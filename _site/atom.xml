<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nrupatunga</title>
    <description></description>
    <link>http://nrupatunga.github.io</link>
    <atom:link
      href="http://nrupatunga.github.io/atom.xml" rel="self"
      type="application/rss+xml" />
    
    <item>
      <title>Net Surgery in Caffe</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script src=&quot;/vendor/MathJax/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Please download the &lt;a href=&quot;/assets/net_surgery_fcn.ipynb&quot;&gt;IPython notebook&lt;/a&gt; from this link. Run the code as you read the post&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is a hands on converting a Convolutional Neural Network (CNN) to Fully Convolution Network (FCN). Converting CNN to FCN is nothing but
converting fully connected (FC) layers in CNN to convolution layers&lt;/p&gt;

&lt;p&gt;Let’s take the standard Caffe Reference ImageNet model &lt;a href=&quot;https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet&quot;&gt;CaffeNet&lt;/a&gt; and
transform it into a fully convolutional net. The code is extracted and modified from the cafee &lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb&quot;&gt;net surgery&lt;/a&gt; example&lt;/p&gt;

&lt;h2 id=&quot;network-architecture&quot;&gt;Network architecture&lt;/h2&gt;
&lt;p&gt;Before converting FC layers to convolution layers, lets see the network architecture first.
Figure 1 shows the architecture of CaffeNet.&lt;/p&gt;

&lt;p&gt;&lt;sub&gt; (Open the image in new tab and zoom to see the details) &lt;/sub&gt;&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/bvlc.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1: CaffeNet architecture&lt;/p&gt;

&lt;/div&gt;

&lt;h4 id=&quot;python-code&quot;&gt;Python Code&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;
  &lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Insert path to caffe, change the path accordingly&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/home/nrupatunga/NThere/Caffe-WS/caffe/python&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;caffe&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Load the original network and extract the fully connected layers&#39; parameters.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caffe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;../models/bvlc_reference_caffenet/deploy.prototxt&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                &lt;span class=&quot;s&quot;&gt;&#39;../models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caffe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TEST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;------------------------------------------------------------------------&#39;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;-------------------------Network Architecture---------------------------&#39;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;------------------------------------------------------------------------&#39;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layername&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layerparam&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;  Layer Name : {0:&amp;gt;7}, Weight Dims :{1:12} &#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layername&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layerparam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;------------------------------------------------------------------------&#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;output&quot;&gt;Output&lt;/h4&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/outputarch.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Execute the python code, output prints the layers in &lt;strong&gt;&lt;em&gt;CaffeNet&lt;/em&gt;&lt;/strong&gt; and dimensions of weights in each
layer. Here for the sake of simplicity I have eliminated bias parameters. As you can see, it has 5 convolution
layers (&lt;em&gt;conv1, conv2, conv3, conv4, conv5&lt;/em&gt;) and three FC (&lt;em&gt;fc6, fc7, fc8&lt;/em&gt;) layers&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lets understand the weight dimensions format in convolution and FC layers. Figure 2 is self explanatory.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/filter.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 2: Weight dimensions in Convolution and FC layers&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Now that we know the network architecture and the weight dimensions in each layer, our intension is to convert FC (&lt;em&gt;fc6, fc7, fc8&lt;/em&gt;) layers to convolution layers&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As explained in Figure 2, in dimensions of &lt;em&gt;fc6&lt;/em&gt; layer \((4096, 9216)\), \(9216\) indicates the number of outputs from convolution layer-5 after pooling.
Lets just confirm that.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;
  &lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;pool5&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/figure&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/pool5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Observe \(256 * 6 * 6\) = \(9216\), so in order to convert &lt;em&gt;fc6&lt;/em&gt; layer to convolution layer, we just need to use the convolution kernel of size 6.
The respective prototxt file change is shown in Figure 4.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/fc6-conv6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 4: fc6 to convolution layer&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;The rest of fully connected layers &lt;em&gt;fc7&lt;/em&gt;, &lt;em&gt;fc8&lt;/em&gt; can be viewed as convolution layer with \(1\) x \(1\) kernel.
The respective prototxt file change is shown in Figure 5.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/fc78.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 5: fc7, fc8 to convolution layer&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Lets verify it now.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;python-code-1&quot;&gt;Python Code&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;
  &lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# Load the fully convolutional network to transplant the parameters.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caffe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;net_surgery/bvlc_caffenet_full_conv.prototxt&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                          &lt;span class=&quot;s&quot;&gt;&#39;../models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;caffe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TEST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;params_full_conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;fc6-conv&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;fc7-conv&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;fc8-conv&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# conv_params = {name: (weights, biases)}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conv_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_full_conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_full_conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;{} weights are {} dimensional&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;output-1&quot;&gt;Output&lt;/h4&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/fc78-output.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&quot;weight-transplant&quot;&gt;Weight transplant&lt;/h2&gt;
&lt;p&gt;Now that we converted the network architecture, lets transfer the
weights from CNN to FCN and generate the classification map.&lt;/p&gt;

&lt;h4 id=&quot;python-code-2&quot;&gt;Python Code&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;
  &lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;fc6&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;fc7&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;fc8&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fc_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pr_conv&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_full_conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;conv_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr_conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flat&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# flat unrolls the arrays&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;conv_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr_conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;net_surgery/bvlc_caffenet_full_conv.caffemodel&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# load input and configure preprocessing&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caffe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;images/cat.jpg&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caffe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;../python/caffe/imagenet/ilsvrc_2012_mean.npy&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_channel_swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_raw_scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;255.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# make classification map by forward and print prediction indices at each location&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;prob&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# show net input and confidence map (probability of the top prediction at each location)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;prob&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;281&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;output-2&quot;&gt;Output&lt;/h4&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/class-output.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;As you can see in the probability map values, the classifications include various cats – 282 = tiger cat, 281 = tabby, 283 = persian.&lt;/p&gt;

&lt;p&gt;So FCN can be used to extract dense feature maps. This enables us dense
learning (eg. Image Semantic Segmenation).&lt;/p&gt;

&lt;p&gt;That is it, we have converted CNN to FCN. It is easy isn’t it?.&lt;/p&gt;
</description>
      <pubDate>
        Wed, 01 Jun 2016 00:00:00 +0530
      </pubDate>
      <link>http://nrupatunga.github.io/2016/06/01/net-surgery-in-caffe/</link>
      <guid isPermaLink="true">http://nrupatunga.github.io/2016/06/01/net-surgery-in-caffe/</guid>
    </item>
    
    <item>
      <title>Image Semantic Segmentation Using Fully Convolutional Neural Network</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script src=&quot;/vendor/MathJax/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post, I will try to give a quick theoretical background to Fully Convolutional neural Network (FCN)
and give a little hands on understanding FCN. Basic knowledge of neural network and convolutional network is assumed.&lt;/p&gt;

&lt;h2 id=&quot;fully-connected-neural-network&quot;&gt;Fully Connected Neural Network&lt;/h2&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/fcn-seg/neural_net.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1: Simple Neural Network&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Figure 1.1 shows a simple neural network with input layer, output layer and one hidden layer.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A neural network is composed of layers, each layer consists of number of neurons.
Each layer is  connected to next layer through some connections which are defined using weights \(W^i\).
A neuron is a computational unit, which takes the input \(X\) and outputs \(Y = f(W*X)\), where \(f()\) is a non-linear function
like &lt;a href=&quot;https://en.wikipedia.org/wiki/Sigmoid_function&quot;&gt;&lt;em&gt;sigmoid&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;&gt;&lt;em&gt;ReLU&lt;/em&gt;&lt;/a&gt;. Neurons are also called as &lt;em&gt;activation units&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Every neuron in a layer is connected to every other neurons in the next layer. These type of networks are called
&lt;strong&gt;&lt;em&gt;Fully connected neural networks&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In fully connected neural network, whatever may be the original input structure, it is converted to 1-D matrix / vector
before it is fed to the input layer. Outputs will also be 1-D vector. This is illustrated in Figure 1.1.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We learn the weights \(W^i\), by backpropogating the errors through intermediate layers in conjunction with
optimization methods such as &lt;em&gt;Gradient Descent&lt;/em&gt; and &lt;em&gt;Stochastic Gradient Descent&lt;/em&gt; algorithm.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;convolutional-neural-network-cnn&quot;&gt;Convolutional Neural Network (CNN)&lt;/h2&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/fcn-seg/cnn.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.2: Convolutional Neural Network&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Figure 1.2 shows the convolutional neural network architecture&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Unlike regular neural networks, the layers in CNN have neurons arranged in 3 dimensions: &lt;strong&gt;&lt;em&gt;width, height&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;depth&lt;/em&gt;&lt;/strong&gt;.
Here &lt;strong&gt;&lt;em&gt;depth&lt;/em&gt;&lt;/strong&gt; refers to the third dimension of the input. For example, in case of color image third dimension is the number of channels&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CNN has three types of layers namely &lt;strong&gt;&lt;em&gt;Convolution&lt;/em&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;Pooling&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;Fully Connected (FC)&lt;/em&gt;&lt;/strong&gt; layers&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Every layer of a CNN transforms the 3-D input volume to a 3-D output volume of neuron activations. This is done using operation called as &lt;strong&gt;&lt;em&gt;Convolution&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;Pooling&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Convolution&lt;/em&gt;&lt;/strong&gt;: It is a linear transformation which respects the ordering of the input data
    &lt;ul&gt;
      &lt;li&gt;Convolution operation is sparse in CNN, i.e., only a few input units contribute to a
  given output unit&lt;/li&gt;
      &lt;li&gt;It reuses the parameters i.e., same weights are applied to
  multiple locations&lt;/li&gt;
      &lt;li&gt;Convolution operation preserves the structure of the input data (eg. Image, Sound clips) unlike fully connected neural
  network where any form of input is always converted to 1-D vector and all the intermediate layers are 1-D vectors.&lt;/li&gt;
      &lt;li&gt;Preserving the input structure helps us to visualize the intermediate layer outputs in CNN and enable us to make useful interpretations and observations in different layers of the network.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Pooling&lt;/em&gt;&lt;/strong&gt;: Pooling operation provide invariance to small translations of the input. Different kinds of pooling
functions are known namely &lt;em&gt;max-pooling&lt;/em&gt; and &lt;em&gt;average pooling&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both &lt;em&gt;Convolution&lt;/em&gt; and &lt;em&gt;Pooling&lt;/em&gt; operations are illustrated in Figure 1.2&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In a typical CNN, final layer will always be a &lt;strong&gt;&lt;em&gt;FC&lt;/em&gt;&lt;/strong&gt; layer as shown in Figure 1.3,
Neurons in a fully connected layer have full connections to all activations in the previous layer,
as seen in regular Neural Networks.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/fcn-seg/conv-net.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.3: Fully connected layers in CNN&lt;/p&gt;

&lt;/div&gt;

&lt;h1 id=&quot;fully-convolutional-neural-network-fcn&quot;&gt;Fully Convolutional Neural Network (FCN)&lt;/h1&gt;
&lt;p&gt;Fully Convolutional Neural Network is nothing but CNN, except that final
FC layer(s) in CNN are converted to convolution layers, as simple as that. Figure 1.3 illustrates this.&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/fcn-seg/CnnToFCN.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.3: Conversion from CNN to FCN&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Transforming FC layers into convolution layers in CNN produces an efficient network for end-to-end dense learning as in
case of Image Segmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let us get a quick hands on how to convert a CNN to FCN using &lt;a href=&quot;http://caffe.berkeleyvision.org/&quot;&gt;Caffe&lt;/a&gt; tool.
Caffe is a Deep learning framework by the &lt;a href=&quot;http://bvlc.eecs.berkeley.edu/&quot;&gt;BVLC&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;hands-on---transforming-cnn-to-fcn&quot;&gt;Hands on - Transforming CNN to FCN&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Let’s take the standard Caffe Reference ImageNet model &lt;strong&gt;&lt;em&gt;CaffeNet&lt;/em&gt;&lt;/strong&gt; and transform it into a fully convolutional
net for efficient, dense inference. The code is extracted from the cafee &lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb&quot;&gt;net surgery&lt;/a&gt; example
Please refer to this &lt;a href=&quot;https://nrupatunga.github.io/netsurgery/&quot;&gt;post&lt;/a&gt; for step by step explanation&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;a name=&quot;Lend&quot;&gt;&lt;/a&gt;
[1] &lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;http://cs231n.github.io/convolutional-networks/&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>
        Wed, 01 Jun 2016 00:00:00 +0530
      </pubDate>
      <link>http://nrupatunga.github.io/2016/06/01/image-semantic-segmentation-using-fully-convolutional-neural-network/</link>
      <guid isPermaLink="true">http://nrupatunga.github.io/2016/06/01/image-semantic-segmentation-using-fully-convolutional-neural-network/</guid>
    </item>
    
    <item>
      <title>Convolution Arithmetic in  Deep Learning Part 2</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script src=&quot;/vendor/MathJax/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;h2 id=&quot;recap-from-part-1&quot;&gt;Recap from Part 1&lt;/h2&gt;

&lt;p&gt;In Part 1, we reviewed the convolution operation and understood how output of convolution is affected by the 
choice of parameters such as &lt;em&gt;kernel size, strides&lt;/em&gt;, and &lt;em&gt;padding&lt;/em&gt;.  We saw the effect of &lt;em&gt;strides&lt;/em&gt; and 
&lt;em&gt;padding&lt;/em&gt; on output of the convolution individually and together.&lt;/p&gt;

&lt;p&gt;At last we came up with a general formula for output of the convolution and its dependency on these parameters.
Please remember the below relationship&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship&lt;/strong&gt;: For any \(i\), \(k\), \(s\), and \(p\) 
&lt;script type=&quot;math/tex&quot;&gt;o = \lfloor \dfrac{i + 2p - k}{s} \rfloor + 1&lt;/script&gt;
where \( i \) : input size, \( k \) : kernel size, \( s \) : stride, \( p \) : zero padding, \(\lfloor \rfloor\): floor operation, \(o\) : output size&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/arbitrary_padding_strides-1.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Figure 2.1  provides an example for \(i = 5\), \(k = 3\), \(s = 2\) and \(p = 1\),
  therefore output size \(o = \lfloor \dfrac{5 + 2*1 - 3}{2} \rfloor + 1 = 5\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that we know how these parameters play a role in convolution, let us understand the same in pooling and deconvolution arithmetic&lt;/p&gt;

&lt;h2 id=&quot;pooling&quot;&gt;Pooling&lt;/h2&gt;

&lt;p&gt;Pooling operation provide invariance to small translations of the input (think in terms of image). Different kinds of pooling
functions are known namely &lt;em&gt;max-pooling&lt;/em&gt; and &lt;em&gt;average pooling&lt;/em&gt;. Pooling does not involves zero padding in neural network. 
So, we can rewrite the formula described in the above section by eliminating the padding term \(p\) as
G
&lt;script type=&quot;math/tex&quot;&gt;o = \lfloor \dfrac{i - k}{s} \rfloor + 1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This relationship holds good for any kind of pooling&lt;/p&gt;

&lt;h2 id=&quot;deconvolution&quot;&gt;Deconvolution&lt;/h2&gt;
&lt;p&gt;Deconvolution is nothing but transpose of convolution. It is also called as &lt;strong&gt;&lt;em&gt;Transposed Convolution&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/no_padding_no_strides.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Lets take an example of convolution represented in Figure 2.2 to understand what I meant. If we unroll the input and output into vectors from left to right and top to bottom,
we can represent the output by the multiplication of input with sparse matrix \(C\), where the non-zero elements are the elements \(w_{i, j}\) 
of the kernel (\(i\) : row and \(j\) : column). This is showin in Figure 2.3&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconvolution-unroll.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;As shown in the figure, matrix multiplication form takes the input matrix flattened as a \(16\)-dimensional vector and produces a \(4\)-dimensional 
vector. Therefore convolution maps the input vector from \(16\)-dimensional space to \(4\)-dimensional space.&lt;/li&gt;
  &lt;li&gt;In order to map back the \(4\)-dimensional space to \(16\)-dimensional space, we multiply the output with \(C^T\). Hence the name &lt;strong&gt;&lt;em&gt;Transposed Convolution&lt;/em&gt;&lt;/strong&gt; for 
&lt;strong&gt;&lt;em&gt;Deconvolution&lt;/em&gt;&lt;/strong&gt;. This is shown in Figure 2.4.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconvolution.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&quot;deconvolution-arithmetic&quot;&gt;Deconvolution Arithmetic&lt;/h2&gt;
&lt;p&gt;In order to analyse deconvolution layer properties, we use the same simplified settings we used for convolution layer.&lt;/p&gt;

&lt;h3 id=&quot;no-zero-padding-unit-strides-transposed&quot;&gt;No Zero Padding, Unit Strides, Transposed&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The example in Figure 2.2 shows convolution of \(3\) x \(3\) kernel on a \(4\) x \(4\) input with 
unitary stride and no padding (i.e., \(i = 4, k = 3, s = 1, p = 0\)). This produces output of size \(2\) x \(2\).&lt;/li&gt;
  &lt;li&gt;The transpose of this convolution is to obtain output of shape \(4\) x \(4\) when applied on a \(2\) x \(2\) input.
This can be achieved by directly convolving \(3\) x \(3\) kernel over \(2\) x \(2\) input padded with a \(2\) x \(2\) border of zeros
(i.e., \(i’= 2, k’ = k, s’ = s, p’ = 2\)). This is shown in Figure 2.5&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.5.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Relationship-7&lt;/strong&gt;: A convolution described by \(s = 1, p = 0\), and \(k\) has an associated deconvolution described by \(k’ = k, s’ = s\)  and \(p’ = k - 1\)
and its output size is 
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;+ k - 1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Proof&lt;/em&gt;&lt;/strong&gt;: We know that, general form of convolution is defined as
&lt;script type=&quot;math/tex&quot;&gt;o = \lfloor \dfrac{i + 2p - k}{s} \rfloor + 1&lt;/script&gt;
subsituting for \(s = 1, p = 0\), convolution is defined as
&lt;script type=&quot;math/tex&quot;&gt;o = i - k + 1&lt;/script&gt;
But, we need \(o = i’, i = o’\), therefore
&lt;script type=&quot;math/tex&quot;&gt;i&#39; = o&#39; - k + 1&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\implies o&#39; = i&#39; + k - 1&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;zero-padding-unit-strides-transposed&quot;&gt;Zero Padding, Unit Strides, Transposed&lt;/h3&gt;
&lt;p&gt;The transpose of zero padded convolution is equivalent to convolving an input padded with less zeros&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship-8&lt;/strong&gt;: A convolution described by \(s = 1, k\), and \(p\) has an associated deconvolution described by \(k’ = k, s’ = s\)  and \(p’ = k - p - 1\)
and its output size is 
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;+ (k - 1) - 2p&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This is illustrated in Figure 2.6, for \(i = 5, k = 4\) and \(p = 2\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.6.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h3 id=&quot;half-padding-transposed&quot;&gt;Half Padding, Transposed&lt;/h3&gt;
&lt;p&gt;Applying the same reasoning as before, transpose of half padded convolution is itself a half padded convolution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship-9&lt;/strong&gt;: A convolution described by \(k = 2n + 1\), \(n \in N\) and \(s = 1\) and \(p = \lfloor \dfrac{k}{2} \rfloor = n\)
has an associated transposed convolution described by \(k’ = k, s’ = s, p’ = p\) and its output size is
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;+ (k - 1) - 2p&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;+ 2n - 2n&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This is illustrated in Figure 2.7, for \(i = 5, k = 3\) and therefore \(p = 1\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.7.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h3 id=&quot;full-padding-transposed&quot;&gt;Full Padding, Transposed&lt;/h3&gt;
&lt;p&gt;The equivalent of fully padded convolution is a non-padded convolution&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship-10&lt;/strong&gt;: A convolution described by \(s = 1, k\) and \(p = k - 1\)
has an associated transposed convolution described by \(k’ = k, s’ = s, p’ = 0\) and its output size is
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;+ (k - 1) - 2p&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;- (k - 1)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This is illustrated in Figure 2.8, for \(i = 5, k = 3\) and therefore \(p = 2\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.8.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h3 id=&quot;no-zero-padding-non-unit-strides-transposed&quot;&gt;No Zero Padding, Non-Unit Strides, Transposed&lt;/h3&gt;
&lt;p&gt;Using the same kind of reasoning as before, we might expect that the transpose of a convolution with \(s &amp;gt; 1\) involves 
equivalent convolution with \(s &amp;lt; 1\). This is a valid intution and which is why transposed convolutions are also called as &lt;em&gt;fractionally
strided convolutions&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship-11&lt;/strong&gt;: A convolution described by \(p = 0, k\) and \(s\) and whose input
size is such that \(i - k\) is a multiple of \(s\), has an associated transposed
convolution described by \(\tilde{i}’\), \(k’ = k\), \(s’ = 1\) and \(p’ = k - 1\),
where \(\tilde{i}’\) is the size of the stretched input obtained by adding
\(s - 1\) zeros between each input unit, and its output size&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;o&#39; = s (i&#39; - 1) + k&lt;/script&gt;

&lt;p&gt;This is illustrated in Figure 2.9, for \(i = 5, k = 3\) and \(s = 2\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.9.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h3 id=&quot;zero-padding-non-unit-strides-transposed&quot;&gt;Zero Padding, Non-Unit Strides, Transposed&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Relationship-12&lt;/strong&gt;: A convolution described by \(k\), \(s\) and \(p\) and whose
input size \(i\) is such that \(i + 2p - k\) is a multiple of \(s\) has an associated
transposed convolution described by \(\tilde{i}’\), \(k’ = k\), \(s’ = 1\) and
\(p’ = k - p - 1\), where \(\tilde{i}’\) is the size of the stretched input
obtained by adding \(s - 1\) zeros between each input unit, and its output size
is&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;o&#39; = s (i&#39; - 1) + k - 2p&lt;/script&gt;
This is illustrated in Figure 2.10, for \(i = 5, k = 3, s = 2\) and \(p = 1\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.10.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;The constraint on the size of the input can be removed by introducing a new parameter \(a \in {0, \ldots, s - 1}\)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship-13&lt;/strong&gt;: A convolution described by \(k\), \(s\) and \(p\) has an
associated transposed convolution described by \(a\), \(\tilde{i}’\), \(k’ = k\), \(s’ = 1\) 
and \(p’ = k - p - 1\), where \(\tilde{i}’\) is the size of the stretched
input obtained by adding \(s - 1\) zeros between each input unit, and \(a = (i + 2p - k) \mod s\)
represents the number of zeros added to the top and right edges
of the input, and its output size is&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;o&#39; = s (i&#39; - 1) + a + k - 2p&lt;/script&gt;
This is illustrated in Figure 2.11, for \(i = 6, k = 3, s = 2\) and \(p = 1\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.11.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&quot;take-away&quot;&gt;Take Away&lt;/h2&gt;

&lt;h3 id=&quot;formula-for-convolution&quot;&gt;Formula for Convolution:&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;o = \lfloor \dfrac{i + 2p - k}{s} \rfloor + 1&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;formula-for-deconvolution&quot;&gt;Formula for Deconvolution:&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;o&#39; = s (i&#39; - 1) + a + k - 2p&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where \( i \) : input size before convolution, \( k \) : kernel size, \( s \) : stride, \( p \) : zero padding, 
\(\lfloor \rfloor\): floor operation, \(o\) : output size after convolution,  \( i’ \) : input size before deconvolution
\(o’\) : output size after deconvolution, \(a \in {0, \ldots, s - 1}\)&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://arxiv.org/abs/1603.07285&quot;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>
        Sun, 15 May 2016 00:00:00 +0530
      </pubDate>
      <link>http://nrupatunga.github.io/convolution-2/</link>
      <guid isPermaLink="true">http://nrupatunga.github.io/convolution-2/</guid>
    </item>
    
    <item>
      <title>Convolution Arithmetic in Deep Learning: Part-1</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script src=&quot;/vendor/MathJax/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Convolutional Neural Networks (&lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;&gt;CNN&lt;/a&gt;) has
become so popular due to its state of the art results in many computer
vision tasks such as &lt;em&gt;Image Recognition&lt;/em&gt;, &lt;em&gt;Image Classification&lt;/em&gt;,
&lt;em&gt;Semantic Segmentation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;For a beginner in Deep Learning, using CNNs for the first time is
generally an intimidating experience. Even though it is easier to
understand the layers of CNNs such as &lt;em&gt;Convolution&lt;/em&gt;,
&lt;em&gt;Pooling&lt;/em&gt;, &lt;em&gt;Non Linear Activations&lt;/em&gt;, &lt;em&gt;Fully Connected layers&lt;/em&gt;,
&lt;em&gt;Deconvolution&lt;/em&gt; when treated individually, its quite difficult to make
sense of effect of these operations on each layer’s output size, shape
as the networks gets deeper&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://arxiv.org/pdf/1505.04366v1.pdf&quot;&gt;Figure 1.1&lt;/a&gt; below shows the
sample CNN architecture. We can see how the network is stacked up with
&lt;em&gt;Convolution&lt;/em&gt;, &lt;em&gt;Pooling&lt;/em&gt;, &lt;em&gt;Deconvolution&lt;/em&gt; layers. At the top of each
layers, you can observe the shape of the output mentioned. How did we
get those numbers?. This is dependent on the parameters choosen in that layer.
That’s what we want to understand.&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/cnn.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.1: CNN architecture&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;I experienced that good understanding of computational mechanism of convolution, pooling and
deconvolution layers and their dependency on parameters such as &lt;em&gt;kernel size, strides, padding&lt;/em&gt;
together builds a solid ground in understanding CNNs better&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The main objectives for rest of the post:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To understand the relationship between input shape, kernel shape, zero
  padding, strides and output shape in convolution, pooling and
  deconvolution layers&lt;/li&gt;
  &lt;li&gt;To understand the relationship between convolution layers and
  deconvolution layers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The main building blocks of CNN are &lt;strong&gt;&lt;em&gt;Discrete Convolution&lt;/em&gt;&lt;/strong&gt; and
&lt;strong&gt;&lt;em&gt;Pooling&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;affine-transformation-and-discrete-convolution&quot;&gt;Affine Transformation and Discrete Convolution&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Affine Transformation&lt;/em&gt;&lt;/strong&gt;: A vector is given as an input and is
  multiplied with a matrix to produce an output. This is the
  transformation which is most often used in Neural networks. This is
  applicable to any type of input be it an image, a sound clip:
  whatever their dimensionality, the representation can always be
  flattened into a vector before transformation
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;If you take &lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;MNIST&lt;/a&gt; digit recognition as an example, the
   2-D input is flattened to a 1-D vector of size and fed as an input to
   the typical &lt;a href=&quot;http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/&quot;&gt;fully connected neural network&lt;/a&gt; as shown in Figure 1.2&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;But we notice images, sound clips have intrinsic structure. They share following properties
        &lt;ul&gt;
          &lt;li&gt;They are stored as multi-dimensional array&lt;/li&gt;
          &lt;li&gt;They feature one or more axes for which ordering
  matters (e.g., width and height axes of an image,
  time axis for a sound clip)&lt;/li&gt;
          &lt;li&gt;One axis, called the channel axis is used to access
  different views of the data (e.g., red, green, and
  blue channels of a color image, or left and right
  channels of a stereo audio track)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;These properties are not exploited when &lt;em&gt;Affine transformation&lt;/em&gt; is applied. All the axes are treated
    in the same way and topological information is not taken into consideration&lt;/li&gt;
      &lt;li&gt;In computer vision tasks, taking the advantage of implicit structure of the input data can be very handy&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/mnist.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.2: Multi-layer perceptron&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Discrete Convolution&lt;/em&gt;&lt;/strong&gt;: It is a linear transformation which respects the ordering of the input data that we discussed above
    &lt;ul&gt;
      &lt;li&gt;Convolution operation is sparse in CNN, i.e., only a few input units contribute to a
  given output unit (Figure 1.3 (a))&lt;/li&gt;
      &lt;li&gt;It reuses the parameters i.e., same weights are applied to
  multiple locations (Figure 1.3 (b))&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/localconnectivity_sharedweights.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.3: (a) Local connectivity (b) Shared weights&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&quot;discrete-convolution-in-detail&quot;&gt;Discrete Convolution in detail&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Figure 1.4 shows an example of a discrete convolution. The light blue
  grid is called the &lt;strong&gt;&lt;em&gt;input feature map&lt;/em&gt;&lt;/strong&gt; and the shaded area is the
  &lt;strong&gt;&lt;em&gt;kernel&lt;/em&gt;&lt;/strong&gt; (&lt;em&gt;kernel values are at right bottom of each cell in shaded area&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/convolution-2.1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.4: Computing the output values of a discrete convolution&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;At each location, the product between the each element of the kernel
  and the input elements which overlaps is computed and results are
  summed up to obtain the output in the current location. The green
  grid in the figure illustrates this, shaded area in the green grid
  indicates output at that current location&lt;/li&gt;
  &lt;li&gt;The convolution operation shown in the Figure 1.4 is an instance of
  2-D convolution, but this can be generalized to N-D convolution.
  For instance, in a 3-D convolution, the kernel would be a
  &lt;em&gt;cuboid&lt;/em&gt; and would slide across the height, width and the depth of
  the input.&lt;/li&gt;
  &lt;li&gt;In CNN, the collection of kernels defining a discrete convolution has a shape
  corresponding to some permutation of (\(n, m, k_j\)),
  where
  &lt;script type=&quot;math/tex&quot;&gt;n \equiv number\  of\ output\ feature\ maps&lt;/script&gt;
  &lt;script type=&quot;math/tex&quot;&gt;m \equiv number\  of\ input\ feature\ maps&lt;/script&gt;
  &lt;script type=&quot;math/tex&quot;&gt;k\_j \equiv kernel\  size\ along\ axis\ j \ (\ width \ or \ height \ axis\ )&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The following properties affect the output size \(o_j\) of a
  convolution layer along the axis \(j\)
  &lt;script type=&quot;math/tex&quot;&gt;i\_j : input\  size\ along\ axis\ j&lt;/script&gt;
  &lt;script type=&quot;math/tex&quot;&gt;k\_j : kernel\  size\ along\ axis\ j&lt;/script&gt;
  &lt;script type=&quot;math/tex&quot;&gt;s\_j : stride\  along\ axis\ j&lt;/script&gt;
  &lt;script type=&quot;math/tex&quot;&gt;p\_j : zero\  padding\ along\ axis\ j&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;For instance Figure 1.5 shows a \(3*3\) kernel applied to
  \(5*5\) input padded with \(1*1\) border of zeros using \(2*2\)
  strides&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/convolution-2.2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.5: Computing the output values of a discrete convolution for
	  \(N=2\)  \(i_1 = i_2 = 5\),  \(k_1 = k_2 = 3\) ,  \(s_1 = s_2 = 2\),  \(p_1 = p_2 = 1\)&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;The strides constitute a form of &lt;strong&gt;&lt;em&gt;subsampling&lt;/em&gt;&lt;/strong&gt;. Strides can be
  viewed as much of the output is retained.  For instance, moving the
  kernel by hops of two is equivalent to moving the kernel by hops
  of one, but retain only odd output elements (Figure 1.6)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/convolution-2.3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.6: An alternative way of viewing strides. Instead of
translating the \(3*3\) kernel by increments of \(s=2\) (left), the kernel is translated by increments of \(1\) and only odd numbered output elements are retained&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&quot;convolutional-arithmetic&quot;&gt;Convolutional Arithmetic&lt;/h2&gt;
&lt;p&gt;The analysis of relationship between convolutional layer properties is
eased by the fact that they don’t interact across axes, i.e., the choice
of kernel size, stride and zero padding along the axis \(j\) only affects
the output size along the axis \(j\)&lt;/p&gt;

&lt;p&gt;The following simplified settings are used to analyse the convolution
layer properties&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(2\)-\(D\) discrete convolutions (\(N = 2\))&lt;/li&gt;
  &lt;li&gt;square inputs (\(i_1 = i_2 = i\))&lt;/li&gt;
  &lt;li&gt;square kernel size (\(k_1 = k_2 = k\))&lt;/li&gt;
  &lt;li&gt;same strides along both axes (\(s_1 = s_2 = s\))&lt;/li&gt;
  &lt;li&gt;same zero padding along both axes (\(p_1 = p_2 = p\))&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;no-zero-padding-unit-strides&quot;&gt;No Zero Padding, Unit Strides&lt;/h3&gt;
&lt;p&gt;The simplest case to analyse is when the kernel just slides across every position of the input (i.e., \(s = 1\) and \(p = 0\))&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/no_padding_no_strides.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.7: No Padding, Unit Strides&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Lets define the output size resulting from this setting&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The output size is the number of possible placements of the
  kernel on the input&lt;/li&gt;
  &lt;li&gt;Lets consider the width axis: the kernel starts on the
  leftmost part of the input feature map and slides by steps
  of one until it touches the right side of the input&lt;/li&gt;
  &lt;li&gt;The size of the output will be equal to number of steps made,
  plus one&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Relationship-1&lt;/strong&gt;: &lt;em&gt;For any \(i\), \(k\), and for \(s=1\) and \(p=0\),&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;o = (i-k)+1&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Figure 1.7 provides an example for \(i=4\), \(k = 3\), \(s=1\), therefore output size \(o = (4-3) + 1 = 2\) along each axis&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;zero-padding-unit-strides&quot;&gt;Zero Padding, Unit Strides&lt;/h3&gt;
&lt;p&gt;Lets consider zero padding only restricting stride \(s = 1\). The effect of
zero padding increases the size of the input from \(i\) to \(i+2p\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/arbitrary_padding_no_strides.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.8: Zero Padding, Unit Strides&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Relationship-2&lt;/strong&gt;: &lt;em&gt;For any \(i\), \(k\), \(p\) and for \(s=1\),&lt;/em&gt;
&lt;script type=&quot;math/tex&quot;&gt;o = (i-k)+ 2p + 1&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Figure 1.8  provides an example for \(i = 5\), \(k = 4\) and \(p = 2\),
  therefore output size \(o = (5-4) + 2*2 + 1 = 6\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;half-padding&quot;&gt;Half padding&lt;/h3&gt;
&lt;p&gt;Some times we require the output size of convolution to be same as input
size (i.e., \(o=i\)). In order for \(o=i\), we use \(p = \lfloor \dfrac{k}{2} \rfloor\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/same_padding_no_strides.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.9: Half Padding, Unit Strides&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Relationship-3&lt;/strong&gt;: For any \(i\), and for odd \(k\) (\(k = 2n + 1\), \(n \in N\)), \(s=1\), and \(p = \lfloor \dfrac{k}{2} \rfloor = n\)
&lt;script type=&quot;math/tex&quot;&gt;o = (i+2\lfloor \dfrac{k}{2} \rfloor)-(k - 1)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;o = (i + 2n - 2n)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;o = i&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Figure 1.9  provides an example for \(i = 5\), \(k = 3\) hence \(p = 2\),
  therefore output size \(o = (5+2*\lfloor \dfrac{3}{2} \rfloor) - (3 - 1) = 5 + 2 * 1 - 2 = 5\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;full-padding&quot;&gt;Full padding&lt;/h3&gt;
&lt;p&gt;Some times we require the output size of convolution to be of larger size than as input. But, convolution always decreases
the size of the output if there is no extra padding to the input, so we can do some extra zero padding to the input.&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/full_padding_no_strides.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.10: Full Padding, Unit Strides&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Relationship-4&lt;/strong&gt;: For any \(i\), and \(k\), \(s=1\), and \(p = k - 1\)
&lt;script type=&quot;math/tex&quot;&gt;o = (i + 2(k - 1) - (k - 1)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;o = (i + (k - 1)&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Figure 1.10  provides an example for \(i = 5\), \(k = 3\) hence \(p = 2\),
  therefore output size \(o = 5 + 3 - 1 = 7\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;no-zero-padding-non-unit-strides&quot;&gt;No zero padding, non-unit strides&lt;/h3&gt;
&lt;p&gt;All relationships which we saw till now are unit-strided convolutions. In order to understand the effect of non-unit strides,
lets ignore padding for now.&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/no_padding_strides.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.11: No zero padding, non-unit Strides&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;As we discussed before, output size can be defined in terms of the number of possible placements of the kernel on the input.
If you consider only width axis, the size of the output is equal to the number of steps made, plus one, accounting for the initial position of the kernel.
The same logic applies for height axis.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Relationship-5&lt;/strong&gt;: For any \(i\), \(k\), \(s\), and  for \(p = 0\)
&lt;script type=&quot;math/tex&quot;&gt;o = \lfloor \dfrac{i - k}{s} \rfloor + 1&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Figure 1.11  provides an example for \(i = 5\), \(k = 3\), \(p = 0\) and \(s = 2\),
  therefore output size \(o = \lfloor \dfrac{5 - 3}{2} \rfloor + 1 = 2\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE&lt;/em&gt;&lt;/strong&gt; : The floor function in relationship-5 accounts for the fact that sometimes input size is such that kernel
would not be able to reach all the input units.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Figure 1.12 illustrates this&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/arbitrary_padding_strides.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.12 Arbitrary padding and Strides&lt;/p&gt;

&lt;/div&gt;

&lt;h3 id=&quot;zero-padding-non-unit-strides&quot;&gt;Zero padding, non-unit strides&lt;/h3&gt;
&lt;p&gt;This is the more general case, convolving over a zero padded input using a non-unit strides.
We can derive by applying relationship-5 on effective input size of \(i + 2p\)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship-6&lt;/strong&gt;: For any \(i\), \(k\), \(s\), and \(p\)
&lt;script type=&quot;math/tex&quot;&gt;o = \lfloor \dfrac{i + 2p - k}{s} \rfloor + 1&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/arbitrary_padding_strides-1.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.13 Arbitrary padding and Strides&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Figure 1.13  provides an example for \(i = 5\), \(k = 3\), \(s = 2\) and \(p = 1\),
  therefore output size \(o = \lfloor \dfrac{5 + 2*1 - 3}{2} \rfloor + 1 = 5\).&lt;/li&gt;
  &lt;li&gt;Figure 1.12  provides an example for \(i = 6\), \(k = 3\), \(s = 2\) and \(p = 1\),
  therefore output size \(o = \lfloor \dfrac{6 + 2*1 - 3}{2} \rfloor + 1 = 5\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Observe that even though both has different size inputs \(i = 5\) and \(i = 6\), output size after convolution is same \(o = 5\) for both.
As discussed before, this is due to the fact that kernel is not able to reach all the input units.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;continued in &lt;a href=&quot;https://nrupatunga.github.io/convolution-2/&quot;&gt;part 2&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://arxiv.org/abs/1603.07285&quot;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>
        Sat, 14 May 2016 00:00:00 +0530
      </pubDate>
      <link>http://nrupatunga.github.io/2016/05/14/convolution-arithmetic-in-deep-learning-part-1/</link>
      <guid isPermaLink="true">http://nrupatunga.github.io/2016/05/14/convolution-arithmetic-in-deep-learning-part-1/</guid>
    </item>
    
  </channel>
</rss>
