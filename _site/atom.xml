<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nrupatunga</title>
    <description></description>
    <link>http://nrupatunga.github.io</link>
    <atom:link
      href="http://nrupatunga.github.io/atom.xml" rel="self"
      type="application/rss+xml" />
    
    <item>
      <title>Visual Object Tracking using Adaptive Correlation Filters</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script src=&quot;/vendor/MathJax/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;TIP&lt;/strong&gt;: Please read the &lt;a href=&quot;http://www.cs.colostate.edu/~draper/papers/bolme_cvpr10.pdf&quot;&gt;paper&lt;/a&gt; once and use the following notes to understand the algorithm better&lt;/p&gt;

&lt;p&gt;The main idea of the paper is to model the appearance of the target
object, frame to frame by constantly updating the correlation filter
trained on example images and using the trained filter to update the
location of the target in current frame&lt;/p&gt;

&lt;p&gt;We go through the code available in the
&lt;strong&gt;&lt;a href=&quot;https://github.com/opencv/opencv/blob/master/samples/python/mosse.py&quot;&gt;link&lt;/a&gt;&lt;/strong&gt;
and understand the implementation. Glancing through the code, I found
that good place to start is by understanding &lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;MOSSE&lt;/code&gt;&lt;/strong&gt; class&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MOSSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getOptimalDFTSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getRectSubPix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Constructor &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt;of &lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;MOSSE&lt;/code&gt;&lt;/strong&gt; class takes &lt;code class=&quot;highlighter-rouge&quot;&gt;frame&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;rect&lt;/code&gt; as inputs, where &lt;code class=&quot;highlighter-rouge&quot;&gt;frame&lt;/code&gt;
is the input frame, &lt;code class=&quot;highlighter-rouge&quot;&gt;rect&lt;/code&gt; is the initial bounding box input marked by
the user that encloses the  object of interest to track in rest of the frames.
The bounding box coordinates&lt;code class=&quot;highlighter-rouge&quot;&gt;x1, y1, x2, y2&lt;/code&gt; are with respect to the frame.&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# map function: map(f, iterable) &amp;lt;==&amp;gt; [f(x) for x in iterable]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getOptimalDFTSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.getOptimalDFTSize(vecsize)&lt;/code&gt;&lt;sup&gt;&lt;a href=&quot;http://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#getoptimaldftsize&quot;&gt;doc&lt;/a&gt;&lt;/sup&gt;
returns the minimum number &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt; that is greater than equal to &lt;code class=&quot;highlighter-rouge&quot;&gt;vecsize&lt;/code&gt;,
so that the DFT of a vector of size &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt; can be processed efficiently.&lt;/p&gt;

&lt;p&gt;Here &lt;code class=&quot;highlighter-rouge&quot;&gt;w, h&lt;/code&gt; are the updated width and height of the bounding box. Once
we find the new &lt;code class=&quot;highlighter-rouge&quot;&gt;w, h&lt;/code&gt; we update the coordinates &lt;code class=&quot;highlighter-rouge&quot;&gt;x1, y1&lt;/code&gt;. Rewrite
&lt;code class=&quot;highlighter-rouge&quot;&gt;(x1+x2-w)/2&lt;/code&gt; as &lt;code class=&quot;highlighter-rouge&quot;&gt;(x2-w+x1)/2&lt;/code&gt; to better understand the math this update&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getRectSubPix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We find the center of bounding box &lt;code class=&quot;highlighter-rouge&quot;&gt;x, y&lt;/code&gt; using updated &lt;code class=&quot;highlighter-rouge&quot;&gt;x1, y1&lt;/code&gt;, but these co-ordinates are with
respect to the &lt;code class=&quot;highlighter-rouge&quot;&gt;frame&lt;/code&gt;. We extract the part of &lt;code class=&quot;highlighter-rouge&quot;&gt;frame&lt;/code&gt; which is centered
at &lt;code class=&quot;highlighter-rouge&quot;&gt;x, y&lt;/code&gt; with width &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt; and height &lt;code class=&quot;highlighter-rouge&quot;&gt;h&lt;/code&gt; using
&lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.getRectSubPix&lt;/code&gt;&lt;sup&gt;&lt;a href=&quot;http://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#getrectsubpix&quot;&gt;doc&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/mosse/images/frame_bb.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.1: Frame with bounding box marked&lt;/p&gt;

&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;The next step in the algorithm is to generate the groundtruth for each
input image &lt;code class=&quot;highlighter-rouge&quot;&gt;img&lt;/code&gt;, and also preprocess the input.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;win&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createHanningWindow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CV_32F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GaussianBlur&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DFT_COMPLEX_OUTPUT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;hr /&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;win&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createHanningWindow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CV_32F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Here &lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.createHanningWindow&lt;/code&gt;&lt;sup&gt;&lt;a href=&quot;http://docs.opencv.org/2.4/modules/imgproc/doc/motion_analysis_and_object_tracking.html&quot;&gt;doc&lt;/a&gt;&lt;/sup&gt; 
generates the hanning windows coefficients of size &lt;code class=&quot;highlighter-rouge&quot;&gt;w, h&lt;/code&gt;. The input
image signal is modulated with this function in order to reduce the edge
effects (high frequency).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Hanning window&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/mosse/images/hanning.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;sup&gt; NOTE::&lt;em&gt;To understand the role of Hanning function, you need to understand
the concept of &lt;strong&gt;DFT leakage&lt;/strong&gt;.  In short, when we deal with digital
signals, if the input signal does not contain frequencies which are
integer multiple of analysis frequencies, DFT leakage happens, This can
be avoided by modulating the input signal with different window
functions such as Hamming, Hanning and rectangular functions before
taking Fourier transform&lt;/em&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GaussianBlur&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Basic idea of the algorithm is to train correlation filter &lt;code class=&quot;highlighter-rouge&quot;&gt;H*&lt;/code&gt;, when
convolved with current input frame should give the new location of the
target. This filter is trained in Fourier domain because its
computationally efficient.&lt;/p&gt;

&lt;p&gt;To train, we need input-groundtruth pair. For each &lt;code class=&quot;highlighter-rouge&quot;&gt;img&lt;/code&gt;, groundtruth
&lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; is Gaussian shaped peak centered on the target. Above code generates
normalized Gaussian with variance &lt;code class=&quot;highlighter-rouge&quot;&gt;2.0&lt;/code&gt; as ground truth &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt; for each
input image &lt;code class=&quot;highlighter-rouge&quot;&gt;img&lt;/code&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Ground truth (Gaussian peak)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/mosse/images/gt.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DFT_COMPLEX_OUTPUT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Since we train the filter in Fourier domain, we take the Fourier
transform of Gaussian generated in the previous step.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Once we generate the ground truth &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt;, now we generate more training
examples lets call &lt;code class=&quot;highlighter-rouge&quot;&gt;num_images=128&lt;/code&gt; in order to get good initial
estimate of &lt;code class=&quot;highlighter-rouge&quot;&gt;H&lt;/code&gt;, which is done by random warping the
input image &lt;code class=&quot;highlighter-rouge&quot;&gt;img&lt;/code&gt; using &lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.warpAffine&lt;/code&gt;&lt;sup&gt;&lt;a href=&quot;http://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#getoptimaldftsize&quot;&gt;doc&lt;/a&gt;&lt;/sup&gt; function.&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnd_warp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DFT_COMPLEX_OUTPUT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mulSpectrums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conjB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mulSpectrums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conjB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.warpAffine&lt;/code&gt; function needs &lt;code class=&quot;highlighter-rouge&quot;&gt;2 x 3&lt;/code&gt; transformation matrix &lt;code class=&quot;highlighter-rouge&quot;&gt;T&lt;/code&gt;, where top &lt;code class=&quot;highlighter-rouge&quot;&gt;T[:2, :2]&lt;/code&gt; is &lt;code class=&quot;highlighter-rouge&quot;&gt;2 x 2&lt;/code&gt;
rotation matrix&lt;sup&gt;&lt;a href=&quot;https://www.wikiwand.com/en/Rotation_matrix&quot;&gt;doc&lt;/a&gt;&lt;/sup&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;T[:, 2]&lt;/code&gt; is &lt;code class=&quot;highlighter-rouge&quot;&gt;2 x 1&lt;/code&gt; is translation matrix.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnd_warp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rnd_warp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;coef&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ang&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ang&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ang&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coef&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;warpAffine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;borderMode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BORDER_REFLECT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;hr /&gt;

&lt;p&gt;The output of warping is shown below&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;input image&lt;/th&gt;
      &lt;th&gt;warped image&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/mosse/images/img_bb.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/mosse/images/warped.gif&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;hr /&gt;

&lt;p&gt;Next step is to preprocess the warped image.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;win&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;hr /&gt;

&lt;p&gt;During preprocessing, input is transformed using a &lt;code class=&quot;highlighter-rouge&quot;&gt;log&lt;/code&gt; function which helps in low contrast
lighting situation, then we normalize the input  by subtracting the mean
and dividing by the standard deviation. The input is also modulated
using the hanning window as we discussed before to reduce the edge
effect. The output of the above code is shown below&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;input image&lt;/th&gt;
      &lt;th&gt;log transformed with reduced edge effect&lt;sup&gt;#&lt;/sup&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/mosse/images/img_bb.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/mosse/images/preprocess_with_han.gif&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;sup&gt;# image values are normalized between &lt;code class=&quot;highlighter-rouge&quot;&gt;0-255&lt;/code&gt; for display purpose &lt;/sup&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Next step includes implementing the equation
&lt;img src=&quot;/assets/mosse/images/mosse_eqn.jpg&quot; alt=&quot;&quot; /&gt;, where &lt;code class=&quot;highlighter-rouge&quot;&gt;G&lt;/code&gt; is Fourier transform of ground
truth &lt;code class=&quot;highlighter-rouge&quot;&gt;g&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mulSpectrums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conjB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mulSpectrums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conjB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;F&lt;/code&gt; is the Fourier transform of input &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt; &lt;sup&gt;(in the code &lt;code class=&quot;highlighter-rouge&quot;&gt;F=A&lt;/code&gt;)&lt;/sup&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt; indicates the complex conjugate and &lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt; indicates the number of
image, which ranges from  &lt;code class=&quot;highlighter-rouge&quot;&gt;1 - 128&lt;/code&gt; in our case&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;self.H1, self.H2&lt;/code&gt; implements the numerator and denominator part respectively, using the 
&lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.mulSpectrums&lt;/code&gt;&lt;sup&gt;&lt;a href=&quot;http://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html&quot;&gt;doc&lt;/a&gt;&lt;/sup&gt;.
&lt;code class=&quot;highlighter-rouge&quot;&gt;conjB=True&lt;/code&gt; option conjugates the second input before mulitplication.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;update_kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;divSpec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;divSpec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Ar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ai&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Br&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Ar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Ai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Br&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Bi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As next step, we get the initial estimation of &lt;code class=&quot;highlighter-rouge&quot;&gt;H*&lt;/code&gt;, by calling
&lt;code class=&quot;highlighter-rouge&quot;&gt;self.update_kernel&lt;/code&gt;. This function calls &lt;code class=&quot;highlighter-rouge&quot;&gt;divSpec&lt;/code&gt; which performs
element wise division grouping real and imaginary part of the data&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.125&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last_img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getRectSubPix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last_resp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;psr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;correlate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;good&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;psr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;8.0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;good&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;

    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last_img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getRectSubPix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DFT_COMPLEX_OUTPUT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mulSpectrums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conjB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mulSpectrums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conjB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;correlate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mulSpectrums&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DFT_COMPLEX_OUTPUT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conjB&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;resp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idft&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DFT_SCALE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DFT_REAL_OUTPUT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minMaxLoc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;side_resp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rectangle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;side_resp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;smean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sstd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;side_resp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;side_resp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;psr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;smean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sstd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;psr&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;So far, we got the initial estimate of &lt;code class=&quot;highlighter-rouge&quot;&gt;H*&lt;/code&gt;. With this &lt;code class=&quot;highlighter-rouge&quot;&gt;H*&lt;/code&gt;, we update
the location of target in current frame. This is done by calling the
function &lt;code class=&quot;highlighter-rouge&quot;&gt;correlate&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;C = cv2.mulSpectrums(cv2.dft(img, flags=cv2.DFT_COMPLEX_OUTPUT), self.H, 0, conjB=True)&lt;/code&gt; correlates the input image with filter. 
This is done in Fourier domain again. &lt;code class=&quot;highlighter-rouge&quot;&gt;resp = cv2.idft(C, flags=cv2.DFT_SCALE | cv2.DFT_REAL_OUTPUT)&lt;/code&gt; takes
the &lt;code class=&quot;highlighter-rouge&quot;&gt;idft&lt;/code&gt; of the &lt;code class=&quot;highlighter-rouge&quot;&gt;FFT&lt;/code&gt; output to get the response in spatial domain.
Then, we find the maximum response and its location using &lt;code class=&quot;highlighter-rouge&quot;&gt;cv2.minMaxLoc&lt;/code&gt;&lt;sup&gt;&lt;a href=&quot;http://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html&quot;&gt;doc&lt;/a&gt;&lt;/sup&gt; in the line &lt;code class=&quot;highlighter-rouge&quot;&gt;_, mval, _, (mx, my) = cv2.minMaxLoc(resp)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We then calculate &lt;strong&gt;Peak-Sidelobe-Ratio (PSR)&lt;/strong&gt;,  which is given by &lt;code class=&quot;highlighter-rouge&quot;&gt;PSR =
(mval - mean_side_lobe) / std_side_lobe&lt;/code&gt;, where &lt;code class=&quot;highlighter-rouge&quot;&gt;mval&lt;/code&gt; is maximum response.
The side lobe considered is &lt;code class=&quot;highlighter-rouge&quot;&gt;11x11&lt;/code&gt; pixel around &lt;code class=&quot;highlighter-rouge&quot;&gt;(mx, my)&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;psr = (mval-smean) / (sstd+eps)&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;eps=1e-5&lt;/code&gt; is used for numerical stability.
If &lt;code class=&quot;highlighter-rouge&quot;&gt;psr &amp;lt;= 8.0&lt;/code&gt;, then the object is considered to be occluded or
tracking has failed.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;H2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_kernel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Once we got the location &lt;code class=&quot;highlighter-rouge&quot;&gt;self.pos&lt;/code&gt; of target for the current frame, we
update our estimate &lt;code class=&quot;highlighter-rouge&quot;&gt;H*&lt;/code&gt; by calculating &lt;code class=&quot;highlighter-rouge&quot;&gt;self.H1&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;self.H2&lt;/code&gt; and
updating the filter with learning rate of &lt;code class=&quot;highlighter-rouge&quot;&gt;0.125&lt;/code&gt;. This continues for
each frame where we update the filter and the location of target hand in
hand.&lt;/p&gt;

&lt;p&gt;Thats how tracking using correlation filter is done!. Hope you could
understand the algorithm better. Thank you for reading.&lt;/p&gt;
</description>
      <pubDate>
        Thu, 14 Sep 2017 00:00:00 +0530
      </pubDate>
      <link>http://nrupatunga.github.io/2017/09/14/Visual-Object-Tracking-using-Adaptive-Correlation-Filters/</link>
      <guid isPermaLink="true">http://nrupatunga.github.io/2017/09/14/Visual-Object-Tracking-using-Adaptive-Correlation-Filters/</guid>
    </item>
    
    <item>
      <title>Net Surgery in Caffe</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script src=&quot;/vendor/MathJax/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Please download the &lt;a href=&quot;/assets/net_surgery_fcn.ipynb&quot;&gt;IPython notebook&lt;/a&gt; from this link. Run the code as you read the post&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is a hands on converting a Convolutional Neural Network (CNN) to Fully Convolution Network (FCN). Converting CNN to FCN is nothing but
converting fully connected (FC) layers in CNN to convolution layers&lt;/p&gt;

&lt;p&gt;Let’s take the standard Caffe Reference ImageNet model &lt;a href=&quot;https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet&quot;&gt;CaffeNet&lt;/a&gt; and
transform it into a fully convolutional net. The code is extracted and modified from the cafee &lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb&quot;&gt;net surgery&lt;/a&gt; example&lt;/p&gt;

&lt;h2 id=&quot;network-architecture&quot;&gt;Network architecture&lt;/h2&gt;
&lt;p&gt;Before converting FC layers to convolution layers, lets see the network architecture first.
Figure 1 shows the architecture of CaffeNet.&lt;/p&gt;

&lt;p&gt;&lt;sub&gt; (Open the image in new tab and zoom to see the details) &lt;/sub&gt;&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/bvlc.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1: CaffeNet architecture&lt;/p&gt;

&lt;/div&gt;

&lt;h4 id=&quot;python-code&quot;&gt;Python Code&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;
  &lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sys&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Insert path to caffe, change the path accordingly&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/home/nrupatunga/NThere/Caffe-WS/caffe/python&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;caffe&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#Load the original network and extract the fully connected layers&#39; parameters.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caffe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;../models/bvlc_reference_caffenet/deploy.prototxt&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                &lt;span class=&quot;s&quot;&gt;&#39;../models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caffe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TEST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;------------------------------------------------------------------------&#39;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;-------------------------Network Architecture---------------------------&#39;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;------------------------------------------------------------------------&#39;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layername&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layerparam&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;  Layer Name : {0:&amp;gt;7}, Weight Dims :{1:12} &#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layername&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layerparam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;------------------------------------------------------------------------&#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;output&quot;&gt;Output&lt;/h4&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/outputarch.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Execute the python code, output prints the layers in &lt;strong&gt;&lt;em&gt;CaffeNet&lt;/em&gt;&lt;/strong&gt; and dimensions of weights in each
layer. Here for the sake of simplicity I have eliminated bias parameters. As you can see, it has 5 convolution
layers (&lt;em&gt;conv1, conv2, conv3, conv4, conv5&lt;/em&gt;) and three FC (&lt;em&gt;fc6, fc7, fc8&lt;/em&gt;) layers&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lets understand the weight dimensions format in convolution and FC layers. Figure 2 is self explanatory.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/filter.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 2: Weight dimensions in Convolution and FC layers&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Now that we know the network architecture and the weight dimensions in each layer, our intension is to convert FC (&lt;em&gt;fc6, fc7, fc8&lt;/em&gt;) layers to convolution layers&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;As explained in Figure 2, in dimensions of &lt;em&gt;fc6&lt;/em&gt; layer \((4096, 9216)\), \(9216\) indicates the number of outputs from convolution layer-5 after pooling.
Lets just confirm that.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;
  &lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;pool5&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/figure&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/pool5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Observe \(256 * 6 * 6\) = \(9216\), so in order to convert &lt;em&gt;fc6&lt;/em&gt; layer to convolution layer, we just need to use the convolution kernel of size 6.
The respective prototxt file change is shown in Figure 4.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/fc6-conv6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 4: fc6 to convolution layer&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;The rest of fully connected layers &lt;em&gt;fc7&lt;/em&gt;, &lt;em&gt;fc8&lt;/em&gt; can be viewed as convolution layer with \(1\) x \(1\) kernel.
The respective prototxt file change is shown in Figure 5.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/fc78.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 5: fc7, fc8 to convolution layer&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Lets verify it now.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;python-code-1&quot;&gt;Python Code&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;
  &lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# Load the fully convolutional network to transplant the parameters.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caffe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;net_surgery/bvlc_caffenet_full_conv.prototxt&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                          &lt;span class=&quot;s&quot;&gt;&#39;../models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;caffe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TEST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;params_full_conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;fc6-conv&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;fc7-conv&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;fc8-conv&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# conv_params = {name: (weights, biases)}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conv_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_full_conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_full_conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;{} weights are {} dimensional&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;output-1&quot;&gt;Output&lt;/h4&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/fc78-output.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&quot;weight-transplant&quot;&gt;Weight transplant&lt;/h2&gt;
&lt;p&gt;Now that we converted the network architecture, lets transfer the
weights from CNN to FCN and generate the classification map.&lt;/p&gt;

&lt;h4 id=&quot;python-code-2&quot;&gt;Python Code&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;
  &lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;fc6&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;fc7&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;fc8&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fc_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pr_conv&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_full_conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;conv_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr_conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flat&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# flat unrolls the arrays&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;conv_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr_conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;net_surgery/bvlc_caffenet_full_conv.caffemodel&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# load input and configure preprocessing&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caffe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;images/cat.jpg&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;caffe&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Transformer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;../python/caffe/imagenet/ilsvrc_2012_mean.npy&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_channel_swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_raw_scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;255.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# make classification map by forward and print prediction indices at each location&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;prob&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# show net input and confidence map (probability of the top prediction at each location)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transformer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;deprocess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net_full_conv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blobs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;prob&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;281&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;output-2&quot;&gt;Output&lt;/h4&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/net-surgery/class-output.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;As you can see in the probability map values, the classifications include various cats – 282 = tiger cat, 281 = tabby, 283 = persian.&lt;/p&gt;

&lt;p&gt;So FCN can be used to extract dense feature maps. This enables us dense
learning (eg. Image Semantic Segmenation).&lt;/p&gt;

&lt;p&gt;That is it, we have converted CNN to FCN. It is easy isn’t it?.&lt;/p&gt;
</description>
      <pubDate>
        Wed, 01 Jun 2016 00:00:00 +0530
      </pubDate>
      <link>http://nrupatunga.github.io/2016/06/01/net-surgery-in-caffe/</link>
      <guid isPermaLink="true">http://nrupatunga.github.io/2016/06/01/net-surgery-in-caffe/</guid>
    </item>
    
    <item>
      <title>Image Semantic Segmentation Using Fully Convolutional Neural Network</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script src=&quot;/vendor/MathJax/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post, I will try to give a quick theoretical background to Fully Convolutional neural Network (FCN)
and give a little hands on understanding FCN. Basic knowledge of neural network and convolutional network is assumed.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;fully-connected-neural-network&quot;&gt;Fully Connected Neural Network&lt;/h2&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/fcn-seg/neural_net.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1: Simple Neural Network&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Figure 1.1 shows a simple neural network with input layer, output layer and one hidden layer.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A neural network is composed of layers, each layer consists of number of neurons.
Each layer is  connected to next layer through some connections which are defined using weights \(W^i\).
A neuron is a computational unit, which takes the input \(X\) and outputs \(Y = f(W*X)\), where \(f()\) is a non-linear function
like &lt;a href=&quot;https://en.wikipedia.org/wiki/Sigmoid_function&quot;&gt;&lt;em&gt;sigmoid&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;&gt;&lt;em&gt;ReLU&lt;/em&gt;&lt;/a&gt;. Neurons are also called as &lt;em&gt;activation units&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Every neuron in a layer is connected to every other neurons in the next layer. These type of networks are called
&lt;strong&gt;&lt;em&gt;Fully connected neural networks&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In fully connected neural network, whatever may be the original input structure, it is converted to 1-D matrix / vector
before it is fed to the input layer. Outputs will also be 1-D vector. This is illustrated in Figure 1.1.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We learn the weights \(W^i\), by backpropogating the errors through intermediate layers in conjunction with
optimization methods such as &lt;em&gt;Gradient Descent&lt;/em&gt; and &lt;em&gt;Stochastic Gradient Descent&lt;/em&gt; algorithm.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;convolutional-neural-network-cnn&quot;&gt;Convolutional Neural Network (CNN)&lt;/h2&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/fcn-seg/cnn.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.2: Convolutional Neural Network&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Figure 1.2 shows the convolutional neural network architecture&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Unlike regular neural networks, the layers in CNN have neurons arranged in 3 dimensions: &lt;strong&gt;&lt;em&gt;width, height&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;depth&lt;/em&gt;&lt;/strong&gt;.
Here &lt;strong&gt;&lt;em&gt;depth&lt;/em&gt;&lt;/strong&gt; refers to the third dimension of the input. For example, in case of color image third dimension is the number of channels&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CNN has three types of layers namely &lt;strong&gt;&lt;em&gt;Convolution&lt;/em&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;Pooling&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;Fully Connected (FC)&lt;/em&gt;&lt;/strong&gt; layers&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Every layer of a CNN transforms the 3-D input volume to a 3-D output volume of neuron activations. This is done using operation called as &lt;strong&gt;&lt;em&gt;Convolution&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;Pooling&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Convolution&lt;/em&gt;&lt;/strong&gt;: It is a linear transformation which respects the ordering of the input data
    &lt;ul&gt;
      &lt;li&gt;Convolution operation is sparse in CNN, i.e., only a few input units contribute to a
  given output unit&lt;/li&gt;
      &lt;li&gt;It reuses the parameters i.e., same weights are applied to
  multiple locations&lt;/li&gt;
      &lt;li&gt;Convolution operation preserves the structure of the input data (eg. Image, Sound clips) unlike fully connected neural
  network where any form of input is always converted to 1-D vector and all the intermediate layers are 1-D vectors.&lt;/li&gt;
      &lt;li&gt;Preserving the input structure helps us to visualize the intermediate layer outputs in CNN and enable us to make useful interpretations and observations in different layers of the network.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Pooling&lt;/em&gt;&lt;/strong&gt;: Pooling operation provide invariance to small translations of the input. Different kinds of pooling
functions are known namely &lt;em&gt;max-pooling&lt;/em&gt; and &lt;em&gt;average pooling&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both &lt;em&gt;Convolution&lt;/em&gt; and &lt;em&gt;Pooling&lt;/em&gt; operations are illustrated in Figure 1.2&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In a typical CNN, final layer will always be a &lt;strong&gt;&lt;em&gt;FC&lt;/em&gt;&lt;/strong&gt; layer as shown in Figure 1.3,
Neurons in a fully connected layer have full connections to all activations in the previous layer,
as seen in regular Neural Networks.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/fcn-seg/conv-net.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.3: Fully connected layers in CNN&lt;/p&gt;

&lt;/div&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;fully-convolutional-neural-network-fcn&quot;&gt;Fully Convolutional Neural Network (FCN)&lt;/h1&gt;
&lt;p&gt;Fully Convolutional Neural Network is nothing but CNN, except that final
FC layer(s) in CNN are converted to convolution layers, as simple as that. Figure 1.3 illustrates this.&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/fcn-seg/CnnToFCN.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.3: Conversion from CNN to FCN&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Transforming FC layers into convolution layers in CNN produces an efficient network for end-to-end dense learning as in
case of Image Segmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let us get a quick hands on how to convert a CNN to FCN using &lt;a href=&quot;http://caffe.berkeleyvision.org/&quot;&gt;Caffe&lt;/a&gt; tool.
Caffe is a Deep learning framework by the &lt;a href=&quot;http://bvlc.eecs.berkeley.edu/&quot;&gt;BVLC&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;hands-on---transforming-cnn-to-fcn&quot;&gt;Hands on - Transforming CNN to FCN&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Let’s take the standard Caffe Reference ImageNet model &lt;strong&gt;&lt;em&gt;CaffeNet&lt;/em&gt;&lt;/strong&gt; and transform it into a fully convolutional
net for efficient, dense inference. The code is extracted from the cafee &lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/examples/net_surgery.ipynb&quot;&gt;net surgery&lt;/a&gt; example
Please refer to this &lt;a href=&quot;https://nrupatunga.github.io/netsurgery/&quot;&gt;post&lt;/a&gt; for step by step explanation&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;a name=&quot;Lend&quot;&gt;&lt;/a&gt;
[1] &lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;http://cs231n.github.io/convolutional-networks/&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>
        Wed, 01 Jun 2016 00:00:00 +0530
      </pubDate>
      <link>http://nrupatunga.github.io/2016/06/01/image-semantic-segmentation-using-fully-convolutional-neural-network/</link>
      <guid isPermaLink="true">http://nrupatunga.github.io/2016/06/01/image-semantic-segmentation-using-fully-convolutional-neural-network/</guid>
    </item>
    
    <item>
      <title>Convolution Arithmetic in  Deep Learning Part 2</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script src=&quot;/vendor/MathJax/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;h2 id=&quot;recap-from-part-1&quot;&gt;Recap from Part 1&lt;/h2&gt;

&lt;p&gt;In Part 1, we reviewed the convolution operation and understood how output of convolution is affected by the 
choice of parameters such as &lt;em&gt;kernel size, strides&lt;/em&gt;, and &lt;em&gt;padding&lt;/em&gt;.  We saw the effect of &lt;em&gt;strides&lt;/em&gt; and 
&lt;em&gt;padding&lt;/em&gt; on output of the convolution individually and together.&lt;/p&gt;

&lt;p&gt;At last we came up with a general formula for output of the convolution and its dependency on these parameters.
Please remember the below relationship&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship&lt;/strong&gt;: For any \(i\), \(k\), \(s\), and \(p\) 
&lt;script type=&quot;math/tex&quot;&gt;o = \lfloor \dfrac{i + 2p - k}{s} \rfloor + 1&lt;/script&gt;
where \( i \) : input size, \( k \) : kernel size, \( s \) : stride, \( p \) : zero padding, \(\lfloor \rfloor\): floor operation, \(o\) : output size&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/arbitrary_padding_strides-1.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Figure 2.1  provides an example for \(i = 5\), \(k = 3\), \(s = 2\) and \(p = 1\),
  therefore output size \(o = \lfloor \dfrac{5 + 2*1 - 3}{2} \rfloor + 1 = 5\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that we know how these parameters play a role in convolution, let us understand the same in pooling and deconvolution arithmetic&lt;/p&gt;

&lt;h2 id=&quot;pooling&quot;&gt;Pooling&lt;/h2&gt;

&lt;p&gt;Pooling operation provide invariance to small translations of the input (think in terms of image). Different kinds of pooling
functions are known namely &lt;em&gt;max-pooling&lt;/em&gt; and &lt;em&gt;average pooling&lt;/em&gt;. Pooling does not involves zero padding in neural network. 
So, we can rewrite the formula described in the above section by eliminating the padding term \(p\) as
G
&lt;script type=&quot;math/tex&quot;&gt;o = \lfloor \dfrac{i - k}{s} \rfloor + 1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This relationship holds good for any kind of pooling&lt;/p&gt;

&lt;h2 id=&quot;deconvolution&quot;&gt;Deconvolution&lt;/h2&gt;
&lt;p&gt;Deconvolution is nothing but transpose of convolution. It is also called as &lt;strong&gt;&lt;em&gt;Transposed Convolution&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/no_padding_no_strides.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Lets take an example of convolution represented in Figure 2.2 to understand what I meant. If we unroll the input and output into vectors from left to right and top to bottom,
we can represent the output by the multiplication of input with sparse matrix \(C\), where the non-zero elements are the elements \(w_{i, j}\) 
of the kernel (\(i\) : row and \(j\) : column). This is showin in Figure 2.3&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconvolution-unroll.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;As shown in the figure, matrix multiplication form takes the input matrix flattened as a \(16\)-dimensional vector and produces a \(4\)-dimensional 
vector. Therefore convolution maps the input vector from \(16\)-dimensional space to \(4\)-dimensional space.&lt;/li&gt;
  &lt;li&gt;In order to map back the \(4\)-dimensional space to \(16\)-dimensional space, we multiply the output with \(C^T\). Hence the name &lt;strong&gt;&lt;em&gt;Transposed Convolution&lt;/em&gt;&lt;/strong&gt; for 
&lt;strong&gt;&lt;em&gt;Deconvolution&lt;/em&gt;&lt;/strong&gt;. This is shown in Figure 2.4.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconvolution.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&quot;deconvolution-arithmetic&quot;&gt;Deconvolution Arithmetic&lt;/h2&gt;
&lt;p&gt;In order to analyse deconvolution layer properties, we use the same simplified settings we used for convolution layer.&lt;/p&gt;

&lt;h3 id=&quot;no-zero-padding-unit-strides-transposed&quot;&gt;No Zero Padding, Unit Strides, Transposed&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The example in Figure 2.2 shows convolution of \(3\) x \(3\) kernel on a \(4\) x \(4\) input with 
unitary stride and no padding (i.e., \(i = 4, k = 3, s = 1, p = 0\)). This produces output of size \(2\) x \(2\).&lt;/li&gt;
  &lt;li&gt;The transpose of this convolution is to obtain output of shape \(4\) x \(4\) when applied on a \(2\) x \(2\) input.
This can be achieved by directly convolving \(3\) x \(3\) kernel over \(2\) x \(2\) input padded with a \(2\) x \(2\) border of zeros
(i.e., \(i’= 2, k’ = k, s’ = s, p’ = 2\)). This is shown in Figure 2.5&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.5.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Relationship-7&lt;/strong&gt;: A convolution described by \(s = 1, p = 0\), and \(k\) has an associated deconvolution described by \(k’ = k, s’ = s\)  and \(p’ = k - 1\)
and its output size is 
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;+ k - 1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Proof&lt;/em&gt;&lt;/strong&gt;: We know that, general form of convolution is defined as
&lt;script type=&quot;math/tex&quot;&gt;o = \lfloor \dfrac{i + 2p - k}{s} \rfloor + 1&lt;/script&gt;
subsituting for \(s = 1, p = 0\), convolution is defined as
&lt;script type=&quot;math/tex&quot;&gt;o = i - k + 1&lt;/script&gt;
But, we need \(o = i’, i = o’\), therefore
&lt;script type=&quot;math/tex&quot;&gt;i&#39; = o&#39; - k + 1&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\implies o&#39; = i&#39; + k - 1&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;zero-padding-unit-strides-transposed&quot;&gt;Zero Padding, Unit Strides, Transposed&lt;/h3&gt;
&lt;p&gt;The transpose of zero padded convolution is equivalent to convolving an input padded with less zeros&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship-8&lt;/strong&gt;: A convolution described by \(s = 1, k\), and \(p\) has an associated deconvolution described by \(k’ = k, s’ = s\)  and \(p’ = k - p - 1\)
and its output size is 
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;+ (k - 1) - 2p&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This is illustrated in Figure 2.6, for \(i = 5, k = 4\) and \(p = 2\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.6.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h3 id=&quot;half-padding-transposed&quot;&gt;Half Padding, Transposed&lt;/h3&gt;
&lt;p&gt;Applying the same reasoning as before, transpose of half padded convolution is itself a half padded convolution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship-9&lt;/strong&gt;: A convolution described by \(k = 2n + 1\), \(n \in N\) and \(s = 1\) and \(p = \lfloor \dfrac{k}{2} \rfloor = n\)
has an associated transposed convolution described by \(k’ = k, s’ = s, p’ = p\) and its output size is
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;+ (k - 1) - 2p&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;+ 2n - 2n&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This is illustrated in Figure 2.7, for \(i = 5, k = 3\) and therefore \(p = 1\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.7.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h3 id=&quot;full-padding-transposed&quot;&gt;Full Padding, Transposed&lt;/h3&gt;
&lt;p&gt;The equivalent of fully padded convolution is a non-padded convolution&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship-10&lt;/strong&gt;: A convolution described by \(s = 1, k\) and \(p = k - 1\)
has an associated transposed convolution described by \(k’ = k, s’ = s, p’ = 0\) and its output size is
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;+ (k - 1) - 2p&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;o&#39; = i&#39;- (k - 1)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This is illustrated in Figure 2.8, for \(i = 5, k = 3\) and therefore \(p = 2\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.8.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h3 id=&quot;no-zero-padding-non-unit-strides-transposed&quot;&gt;No Zero Padding, Non-Unit Strides, Transposed&lt;/h3&gt;
&lt;p&gt;Using the same kind of reasoning as before, we might expect that the transpose of a convolution with \(s &amp;gt; 1\) involves 
equivalent convolution with \(s &amp;lt; 1\). This is a valid intution and which is why transposed convolutions are also called as &lt;em&gt;fractionally
strided convolutions&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship-11&lt;/strong&gt;: A convolution described by \(p = 0, k\) and \(s\) and whose input
size is such that \(i - k\) is a multiple of \(s\), has an associated transposed
convolution described by \(\tilde{i}’\), \(k’ = k\), \(s’ = 1\) and \(p’ = k - 1\),
where \(\tilde{i}’\) is the size of the stretched input obtained by adding
\(s - 1\) zeros between each input unit, and its output size&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;o&#39; = s (i&#39; - 1) + k&lt;/script&gt;

&lt;p&gt;This is illustrated in Figure 2.9, for \(i = 5, k = 3\) and \(s = 2\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.9.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h3 id=&quot;zero-padding-non-unit-strides-transposed&quot;&gt;Zero Padding, Non-Unit Strides, Transposed&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Relationship-12&lt;/strong&gt;: A convolution described by \(k\), \(s\) and \(p\) and whose
input size \(i\) is such that \(i + 2p - k\) is a multiple of \(s\) has an associated
transposed convolution described by \(\tilde{i}’\), \(k’ = k\), \(s’ = 1\) and
\(p’ = k - p - 1\), where \(\tilde{i}’\) is the size of the stretched input
obtained by adding \(s - 1\) zeros between each input unit, and its output size
is&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;o&#39; = s (i&#39; - 1) + k - 2p&lt;/script&gt;
This is illustrated in Figure 2.10, for \(i = 5, k = 3, s = 2\) and \(p = 1\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.10.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;The constraint on the size of the input can be removed by introducing a new parameter \(a \in {0, \ldots, s - 1}\)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship-13&lt;/strong&gt;: A convolution described by \(k\), \(s\) and \(p\) has an
associated transposed convolution described by \(a\), \(\tilde{i}’\), \(k’ = k\), \(s’ = 1\) 
and \(p’ = k - p - 1\), where \(\tilde{i}’\) is the size of the stretched
input obtained by adding \(s - 1\) zeros between each input unit, and \(a = (i + 2p - k) \mod s\)
represents the number of zeros added to the top and right edges
of the input, and its output size is&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;o&#39; = s (i&#39; - 1) + a + k - 2p&lt;/script&gt;
This is illustrated in Figure 2.11, for \(i = 6, k = 3, s = 2\) and \(p = 1\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;../assets/convolution/deconv2.11.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&quot;take-away&quot;&gt;Take Away&lt;/h2&gt;

&lt;h3 id=&quot;formula-for-convolution&quot;&gt;Formula for Convolution:&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;o = \lfloor \dfrac{i + 2p - k}{s} \rfloor + 1&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;formula-for-deconvolution&quot;&gt;Formula for Deconvolution:&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;o&#39; = s (i&#39; - 1) + a + k - 2p&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where \( i \) : input size before convolution, \( k \) : kernel size, \( s \) : stride, \( p \) : zero padding, 
\(\lfloor \rfloor\): floor operation, \(o\) : output size after convolution,  \( i’ \) : input size before deconvolution
\(o’\) : output size after deconvolution, \(a \in {0, \ldots, s - 1}\)&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://arxiv.org/abs/1603.07285&quot;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>
        Sun, 15 May 2016 00:00:00 +0530
      </pubDate>
      <link>http://nrupatunga.github.io/convolution-2/</link>
      <guid isPermaLink="true">http://nrupatunga.github.io/convolution-2/</guid>
    </item>
    
    <item>
      <title>Convolution Arithmetic in Deep Learning: Part-1</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script src=&quot;/vendor/MathJax/MathJax.js?config=TeX-AMS_HTML&quot;&gt;&lt;/script&gt;

&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Convolutional Neural Networks (&lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;&gt;CNN&lt;/a&gt;) has
become so popular due to its state of the art results in many computer
vision tasks such as &lt;em&gt;Image Recognition&lt;/em&gt;, &lt;em&gt;Image Classification&lt;/em&gt;,
&lt;em&gt;Semantic Segmentation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;For a beginner in Deep Learning, using CNNs for the first time is
generally an intimidating experience. Even though it is easier to
understand the layers of CNNs such as &lt;em&gt;Convolution&lt;/em&gt;,
&lt;em&gt;Pooling&lt;/em&gt;, &lt;em&gt;Non Linear Activations&lt;/em&gt;, &lt;em&gt;Fully Connected layers&lt;/em&gt;,
&lt;em&gt;Deconvolution&lt;/em&gt; when treated individually, its quite difficult to make
sense of effect of these operations on each layer’s output size, shape
as the networks gets deeper&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://arxiv.org/pdf/1505.04366v1.pdf&quot;&gt;Figure 1.1&lt;/a&gt; below shows the
sample CNN architecture. We can see how the network is stacked up with
&lt;em&gt;Convolution&lt;/em&gt;, &lt;em&gt;Pooling&lt;/em&gt;, &lt;em&gt;Deconvolution&lt;/em&gt; layers. At the top of each
layers, you can observe the shape of the output mentioned. How did we
get those numbers?. This is dependent on the parameters choosen in that layer.
That’s what we want to understand.&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/cnn.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.1: CNN architecture&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;I experienced that good understanding of computational mechanism of convolution, pooling and
deconvolution layers and their dependency on parameters such as &lt;em&gt;kernel size, strides, padding&lt;/em&gt;
together builds a solid ground in understanding CNNs better&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The main objectives for rest of the post:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To understand the relationship between input shape, kernel shape, zero
  padding, strides and output shape in convolution, pooling and
  deconvolution layers&lt;/li&gt;
  &lt;li&gt;To understand the relationship between convolution layers and
  deconvolution layers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The main building blocks of CNN are &lt;strong&gt;&lt;em&gt;Discrete Convolution&lt;/em&gt;&lt;/strong&gt; and
&lt;strong&gt;&lt;em&gt;Pooling&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;affine-transformation-and-discrete-convolution&quot;&gt;Affine Transformation and Discrete Convolution&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Affine Transformation&lt;/em&gt;&lt;/strong&gt;: A vector is given as an input and is
  multiplied with a matrix to produce an output. This is the
  transformation which is most often used in Neural networks. This is
  applicable to any type of input be it an image, a sound clip:
  whatever their dimensionality, the representation can always be
  flattened into a vector before transformation
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;If you take &lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;MNIST&lt;/a&gt; digit recognition as an example, the
   2-D input is flattened to a 1-D vector of size and fed as an input to
   the typical &lt;a href=&quot;http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/&quot;&gt;fully connected neural network&lt;/a&gt; as shown in Figure 1.2&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;But we notice images, sound clips have intrinsic structure. They share following properties
        &lt;ul&gt;
          &lt;li&gt;They are stored as multi-dimensional array&lt;/li&gt;
          &lt;li&gt;They feature one or more axes for which ordering
  matters (e.g., width and height axes of an image,
  time axis for a sound clip)&lt;/li&gt;
          &lt;li&gt;One axis, called the channel axis is used to access
  different views of the data (e.g., red, green, and
  blue channels of a color image, or left and right
  channels of a stereo audio track)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;These properties are not exploited when &lt;em&gt;Affine transformation&lt;/em&gt; is applied. All the axes are treated
    in the same way and topological information is not taken into consideration&lt;/li&gt;
      &lt;li&gt;In computer vision tasks, taking the advantage of implicit structure of the input data can be very handy&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/mnist.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.2: Multi-layer perceptron&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Discrete Convolution&lt;/em&gt;&lt;/strong&gt;: It is a linear transformation which respects the ordering of the input data that we discussed above
    &lt;ul&gt;
      &lt;li&gt;Convolution operation is sparse in CNN, i.e., only a few input units contribute to a
  given output unit (Figure 1.3 (a))&lt;/li&gt;
      &lt;li&gt;It reuses the parameters i.e., same weights are applied to
  multiple locations (Figure 1.3 (b))&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/localconnectivity_sharedweights.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.3: (a) Local connectivity (b) Shared weights&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&quot;discrete-convolution-in-detail&quot;&gt;Discrete Convolution in detail&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Figure 1.4 shows an example of a discrete convolution. The light blue
  grid is called the &lt;strong&gt;&lt;em&gt;input feature map&lt;/em&gt;&lt;/strong&gt; and the shaded area is the
  &lt;strong&gt;&lt;em&gt;kernel&lt;/em&gt;&lt;/strong&gt; (&lt;em&gt;kernel values are at right bottom of each cell in shaded area&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/convolution-2.1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.4: Computing the output values of a discrete convolution&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;At each location, the product between the each element of the kernel
  and the input elements which overlaps is computed and results are
  summed up to obtain the output in the current location. The green
  grid in the figure illustrates this, shaded area in the green grid
  indicates output at that current location&lt;/li&gt;
  &lt;li&gt;The convolution operation shown in the Figure 1.4 is an instance of
  2-D convolution, but this can be generalized to N-D convolution.
  For instance, in a 3-D convolution, the kernel would be a
  &lt;em&gt;cuboid&lt;/em&gt; and would slide across the height, width and the depth of
  the input.&lt;/li&gt;
  &lt;li&gt;In CNN, the collection of kernels defining a discrete convolution has a shape
  corresponding to some permutation of (\(n, m, k_j\)),
  where
  &lt;script type=&quot;math/tex&quot;&gt;n \equiv number\  of\ output\ feature\ maps&lt;/script&gt;
  &lt;script type=&quot;math/tex&quot;&gt;m \equiv number\  of\ input\ feature\ maps&lt;/script&gt;
  &lt;script type=&quot;math/tex&quot;&gt;k\_j \equiv kernel\  size\ along\ axis\ j \ (\ width \ or \ height \ axis\ )&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The following properties affect the output size \(o_j\) of a
  convolution layer along the axis \(j\)
  &lt;script type=&quot;math/tex&quot;&gt;i\_j : input\  size\ along\ axis\ j&lt;/script&gt;
  &lt;script type=&quot;math/tex&quot;&gt;k\_j : kernel\  size\ along\ axis\ j&lt;/script&gt;
  &lt;script type=&quot;math/tex&quot;&gt;s\_j : stride\  along\ axis\ j&lt;/script&gt;
  &lt;script type=&quot;math/tex&quot;&gt;p\_j : zero\  padding\ along\ axis\ j&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;For instance Figure 1.5 shows a \(3*3\) kernel applied to
  \(5*5\) input padded with \(1*1\) border of zeros using \(2*2\)
  strides&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/convolution-2.2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.5: Computing the output values of a discrete convolution for
	  \(N=2\)  \(i_1 = i_2 = 5\),  \(k_1 = k_2 = 3\) ,  \(s_1 = s_2 = 2\),  \(p_1 = p_2 = 1\)&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;The strides constitute a form of &lt;strong&gt;&lt;em&gt;subsampling&lt;/em&gt;&lt;/strong&gt;. Strides can be
  viewed as much of the output is retained.  For instance, moving the
  kernel by hops of two is equivalent to moving the kernel by hops
  of one, but retain only odd output elements (Figure 1.6)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/convolution-2.3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.6: An alternative way of viewing strides. Instead of
translating the \(3*3\) kernel by increments of \(s=2\) (left), the kernel is translated by increments of \(1\) and only odd numbered output elements are retained&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&quot;convolutional-arithmetic&quot;&gt;Convolutional Arithmetic&lt;/h2&gt;
&lt;p&gt;The analysis of relationship between convolutional layer properties is
eased by the fact that they don’t interact across axes, i.e., the choice
of kernel size, stride and zero padding along the axis \(j\) only affects
the output size along the axis \(j\)&lt;/p&gt;

&lt;p&gt;The following simplified settings are used to analyse the convolution
layer properties&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(2\)-\(D\) discrete convolutions (\(N = 2\))&lt;/li&gt;
  &lt;li&gt;square inputs (\(i_1 = i_2 = i\))&lt;/li&gt;
  &lt;li&gt;square kernel size (\(k_1 = k_2 = k\))&lt;/li&gt;
  &lt;li&gt;same strides along both axes (\(s_1 = s_2 = s\))&lt;/li&gt;
  &lt;li&gt;same zero padding along both axes (\(p_1 = p_2 = p\))&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;no-zero-padding-unit-strides&quot;&gt;No Zero Padding, Unit Strides&lt;/h3&gt;
&lt;p&gt;The simplest case to analyse is when the kernel just slides across every position of the input (i.e., \(s = 1\) and \(p = 0\))&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/no_padding_no_strides.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.7: No Padding, Unit Strides&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Lets define the output size resulting from this setting&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The output size is the number of possible placements of the
  kernel on the input&lt;/li&gt;
  &lt;li&gt;Lets consider the width axis: the kernel starts on the
  leftmost part of the input feature map and slides by steps
  of one until it touches the right side of the input&lt;/li&gt;
  &lt;li&gt;The size of the output will be equal to number of steps made,
  plus one&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Relationship-1&lt;/strong&gt;: &lt;em&gt;For any \(i\), \(k\), and for \(s=1\) and \(p=0\),&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;o = (i-k)+1&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Figure 1.7 provides an example for \(i=4\), \(k = 3\), \(s=1\), therefore output size \(o = (4-3) + 1 = 2\) along each axis&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;zero-padding-unit-strides&quot;&gt;Zero Padding, Unit Strides&lt;/h3&gt;
&lt;p&gt;Lets consider zero padding only restricting stride \(s = 1\). The effect of
zero padding increases the size of the input from \(i\) to \(i+2p\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/arbitrary_padding_no_strides.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.8: Zero Padding, Unit Strides&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Relationship-2&lt;/strong&gt;: &lt;em&gt;For any \(i\), \(k\), \(p\) and for \(s=1\),&lt;/em&gt;
&lt;script type=&quot;math/tex&quot;&gt;o = (i-k)+ 2p + 1&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Figure 1.8  provides an example for \(i = 5\), \(k = 4\) and \(p = 2\),
  therefore output size \(o = (5-4) + 2*2 + 1 = 6\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;half-padding&quot;&gt;Half padding&lt;/h3&gt;
&lt;p&gt;Some times we require the output size of convolution to be same as input
size (i.e., \(o=i\)). In order for \(o=i\), we use \(p = \lfloor \dfrac{k}{2} \rfloor\)&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/same_padding_no_strides.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.9: Half Padding, Unit Strides&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Relationship-3&lt;/strong&gt;: For any \(i\), and for odd \(k\) (\(k = 2n + 1\), \(n \in N\)), \(s=1\), and \(p = \lfloor \dfrac{k}{2} \rfloor = n\)
&lt;script type=&quot;math/tex&quot;&gt;o = (i+2\lfloor \dfrac{k}{2} \rfloor)-(k - 1)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;o = (i + 2n - 2n)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;o = i&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Figure 1.9  provides an example for \(i = 5\), \(k = 3\) hence \(p = 2\),
  therefore output size \(o = (5+2*\lfloor \dfrac{3}{2} \rfloor) - (3 - 1) = 5 + 2 * 1 - 2 = 5\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;full-padding&quot;&gt;Full padding&lt;/h3&gt;
&lt;p&gt;Some times we require the output size of convolution to be of larger size than as input. But, convolution always decreases
the size of the output if there is no extra padding to the input, so we can do some extra zero padding to the input.&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/full_padding_no_strides.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.10: Full Padding, Unit Strides&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Relationship-4&lt;/strong&gt;: For any \(i\), and \(k\), \(s=1\), and \(p = k - 1\)
&lt;script type=&quot;math/tex&quot;&gt;o = (i + 2(k - 1) - (k - 1)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;o = (i + (k - 1)&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Figure 1.10  provides an example for \(i = 5\), \(k = 3\) hence \(p = 2\),
  therefore output size \(o = 5 + 3 - 1 = 7\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;no-zero-padding-non-unit-strides&quot;&gt;No zero padding, non-unit strides&lt;/h3&gt;
&lt;p&gt;All relationships which we saw till now are unit-strided convolutions. In order to understand the effect of non-unit strides,
lets ignore padding for now.&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/no_padding_strides.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.11: No zero padding, non-unit Strides&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;As we discussed before, output size can be defined in terms of the number of possible placements of the kernel on the input.
If you consider only width axis, the size of the output is equal to the number of steps made, plus one, accounting for the initial position of the kernel.
The same logic applies for height axis.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Relationship-5&lt;/strong&gt;: For any \(i\), \(k\), \(s\), and  for \(p = 0\)
&lt;script type=&quot;math/tex&quot;&gt;o = \lfloor \dfrac{i - k}{s} \rfloor + 1&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Figure 1.11  provides an example for \(i = 5\), \(k = 3\), \(p = 0\) and \(s = 2\),
  therefore output size \(o = \lfloor \dfrac{5 - 3}{2} \rfloor + 1 = 2\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE&lt;/em&gt;&lt;/strong&gt; : The floor function in relationship-5 accounts for the fact that sometimes input size is such that kernel
would not be able to reach all the input units.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Figure 1.12 illustrates this&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/arbitrary_padding_strides.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.12 Arbitrary padding and Strides&lt;/p&gt;

&lt;/div&gt;

&lt;h3 id=&quot;zero-padding-non-unit-strides&quot;&gt;Zero padding, non-unit strides&lt;/h3&gt;
&lt;p&gt;This is the more general case, convolving over a zero padded input using a non-unit strides.
We can derive by applying relationship-5 on effective input size of \(i + 2p\)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship-6&lt;/strong&gt;: For any \(i\), \(k\), \(s\), and \(p\)
&lt;script type=&quot;math/tex&quot;&gt;o = \lfloor \dfrac{i + 2p - k}{s} \rfloor + 1&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;image-wrapper&quot;&gt;

  &lt;p&gt;&lt;img src=&quot;/assets/arbitrary_padding_strides-1.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

  &lt;p class=&quot;image-caption&quot;&gt;Figure 1.13 Arbitrary padding and Strides&lt;/p&gt;

&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Figure 1.13  provides an example for \(i = 5\), \(k = 3\), \(s = 2\) and \(p = 1\),
  therefore output size \(o = \lfloor \dfrac{5 + 2*1 - 3}{2} \rfloor + 1 = 5\).&lt;/li&gt;
  &lt;li&gt;Figure 1.12  provides an example for \(i = 6\), \(k = 3\), \(s = 2\) and \(p = 1\),
  therefore output size \(o = \lfloor \dfrac{6 + 2*1 - 3}{2} \rfloor + 1 = 5\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Observe that even though both has different size inputs \(i = 5\) and \(i = 6\), output size after convolution is same \(o = 5\) for both.
As discussed before, this is due to the fact that kernel is not able to reach all the input units.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;continued in &lt;a href=&quot;https://nrupatunga.github.io/convolution-2/&quot;&gt;part 2&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://arxiv.org/abs/1603.07285&quot;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt;&lt;/p&gt;
</description>
      <pubDate>
        Sat, 14 May 2016 00:00:00 +0530
      </pubDate>
      <link>http://nrupatunga.github.io/2016/05/14/convolution-arithmetic-in-deep-learning-part-1/</link>
      <guid isPermaLink="true">http://nrupatunga.github.io/2016/05/14/convolution-arithmetic-in-deep-learning-part-1/</guid>
    </item>
    
  </channel>
</rss>
