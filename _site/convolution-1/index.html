<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Convolution Arithmetic in  Deep Learning Part 1 &middot; NRUPATUNGA
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700|PT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-08 sidebar-overlay">
    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">
<!--<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox" checked>-->
<!--<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox" >-->

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    <p>This is my personal website, where you can find out what am I currently doing in my life. <br><br>I write articles on Computer Vision, Machine Learning and Deep Learning with a good mixture of theory and hands on.</p>
  </div>

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about/">About me</a>
        
      
    
      
    
      
        
          <a class="sidebar-nav-item" href="/cv/">CV</a>
        
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/project/">Projects & Software</a>
        
      
    
      
        
      
    

  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2016. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <label for="sidebar-checkbox" class="sidebar-toggle"></label>

          <h3 class="masthead-title">
            <a href="/" title="Home">NRUPATUNGA</a>
            <small>Computer Vision & Machine Learning Enthusiast</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Convolution Arithmetic in  Deep Learning Part 1</h1>
  <span class="post-date">14 May 2016</span>
  <p>Table of Contents:</p>

<ul>
<li>  <a href="#L1">Introduction</a></li>
<li>  <a href="#L2">Affine Transformation and Discrete convolution</a></li>
<li>  <a href="#L3">Discrete Convolution in detail</a></li>
<li>  <a href="#L4">Convolutional Arithmetic</a>

<ul>
<li><a href="#L41">No Zero Padding, Unit Strides</a></li>
<li><a href="#L42">Zero Padding, Unit Strides</a></li>
<li><a href="#L43">Half Padding</a></li>
<li><a href="#L44">Full Padding</a></li>
<li><a href="#L45">No Zero Padding, non-unit Strides</a></li>
<li><a href="#L46">Zero Padding, non-unit Strides</a> </li>
</ul></li>
<li><a href="#LR">References</a></li>
</ul>

<p><a name="L1"></a></p>

<h2>Introduction</h2>

<p>Convolutional Neural Networks (<a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a>) has
become so popular due to its state of the art results in many computer
vision tasks such as <em>Image Recognition</em>, <em>Image Classification</em>,
<em>Semantic Segmentation</em>.</p>

<p>For a beginner in Deep Learning, using CNNs for the first time is
generally an intimidating experience. Even though it is easier to
understand the layers of CNNs such as <em>Convolution</em>,
<em>Pooling</em>, <em>Non Linear Activations</em>, <em>Fully Connected layers</em>,
<em>Deconvolution</em> when treated individually, its quite difficult to make 
sense of effect of these operations on each layer&#39;s output size, shape 
as the networks gets deeper</p>

<p><a href="http://arxiv.org/pdf/1505.04366v1.pdf">Figure 1.1</a> below shows the
sample CNN architecture. We can see how the network is stacked up with
<em>Convolution</em>, <em>Pooling</em>, <em>Deconvolution</em> layers. At the top of each
layers, you can observe the shape of the output mentioned. How did we
get those numbers?. This is dependent on the parameters choosen in that layer.
That&#39;s what we want to understand.
<table class="image">
<caption align="bottom">Figure 1.1: CNN architecture</caption>
<tr><td><img src="../assets/cnn.jpg" style="max-width: 100%; height: auto;" alt="Figure 1.1: CNN architecture"/></td></tr>
</table></p>

<p>I experienced that good understanding of computational mechanism of convolution, pooling and 
deconvolution layers and their dependency on parameters such as <em>kernel size, strides, padding</em> 
together builds a solid ground in understanding CNNs better</p>

<p><strong><em>The main objectives for rest of the post:</em></strong></p>

<ul>
<li>To understand the relationship between input shape, kernel shape, zero
padding, strides and output shape in convolution, pooling and
deconvolution layers</li>
<li>To understand the relationship between convolution layers and
deconvolution layers</li>
</ul>

<p>The main building blocks of CNN are <strong><em>Discrete Convolution</em></strong> and
<strong><em>Pooling</em></strong>. </p>

<p><a name="L2"></a></p>

<h2>Affine Transformation and Discrete Convolution</h2>

<ul>
<li><p><strong><em>Affine Transformation</em></strong>: A vector is given as an input and is
multiplied with a matrix to produce an output. This is the
transformation which is most often used in Neural networks. This is
applicable to any type of input be it an image, a sound clip:
whatever their dimensionality, the representation can always be
flattened into a vector before transformation</p>

<ul>
<li>If you take <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> digit recognition as an example, the
2-D input is flattened to a 1-D vector of size and fed as an input to
the typical <a href="http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/">fully connected neural network</a> as shown in Figure 1.2
<table class="image">
<caption align="bottom">Figure 1.2: Multi-layer perceptron</caption>
<tr><td><img src="../assets/mnist.png" style="max-width: 100%; height: auto;" alt="Figure 1.2: Multi-layer perceptron"/></td></tr>
</table></li>
<li>But we notice images, sound clips have intrinsic structure. They share following properties

<ul>
<li>They are stored as multi-dimensional array</li>
<li>They feature one or more axes for which ordering
matters (e.g., width and height axes of an image,
time axis for a sound clip)</li>
<li>One axis, called the channel axis is used to access
different views of the data (e.g., red, green, and
blue channels of a color image, or left and right
channels of a stereo audio track)</li>
</ul></li>
<li>These properties are not exploited when <em>Affine transformation</em> is applied. All the axes are treated
  in the same way and topological information is not taken into consideration</li>
<li>In computer vision tasks, taking the advantage of implicit structure of the input data can be very handy</li>
</ul></li>
<li><p><strong><em>Discrete Convolution</em></strong>: It is a linear transformation which respects the ordering of the input data that we discussed above</p>

<ul>
<li>Convolution operation is sparse in CNN, i.e., only a few input units contribute to a
given output unit (Figure 1.3 (a))</li>
<li>It reuses the parameters i.e., same weights are applied to
multiple locations (Figure 1.3 (b))
<table class="image">
<caption align="bottom">Figure 1.3: (a) Local connectivity (b) Shared weights</caption>
<tr><td><img src="../assets/localconnectivity_sharedweights.png" style="max-width: 100%; height: auto;" alt="Figure 1.3: (a) Local connectivity (b) Shared weights"/></td></tr>
</table></li>
</ul></li>
</ul>

<p><a name="L3"></a></p>

<h2>Discrete Convolution in detail</h2>

<ul>
<li><p>Figure 1.4 shows an example of a discrete convolution. The light blue
grid is called the <strong><em>input feature map</em></strong> and the shaded area is the
<strong><em>kernel</em></strong> (<em>kernel values are at right bottom of each cell in shaded area</em>)
<table class="image">
<caption align="bottom">Figure 1.4: Computing the output values of a discrete convolution</caption>
<tr><td><img src="../assets/convolution-2.1.png" style="max-width: 100%; height: auto;" alt="Figure 1.4: Computing the output values of a discrete convolution"/></td></tr>
</table></p></li>
<li><p>At each location, the product between the each element of the kernel
and the input elements which overlaps is computed and results are
summed up to obtain the output in the current location. The green
grid in the figure illustrates this, shaded area in the green grid
indicates output at that current location</p></li>
<li><p>The convolution operation shown in the Figure 1.4 is an instance of
2-D convolution, but this can be generalized to N-D convolution. 
For instance, in a 3-D convolution, the kernel would be a
<em>cuboid</em> and would slide across the height, width and the depth of
the input.</p></li>
<li><p>In CNN, the collection of kernels defining a discrete convolution has a shape
corresponding to some permutation of (\(n, m, k_j\)),
where 
$$ n \equiv number\  of\ output\ feature\ maps$$
$$ m \equiv number\  of\ input\ feature\ maps$$
$$ k_j \equiv kernel\  size\ along\ axis\ j \ (\ width \ or \ height \ axis\ )$$</p></li>
<li><p>The following properties affect the output size \(o_j\) of a
convolution layer along the axis \(j\)
$$ i_j : input\  size\ along\ axis\ j$$
$$ k_j : kernel\  size\ along\ axis\ j$$
$$ s_j : stride\  along\ axis\ j$$
$$ p_j : zero\  padding\ along\ axis\ j$$</p></li>
<li><p>For instance Figure 1.5 shows a \(3*3\) kernel applied to
\(5*5\) input padded with \(1*1\) border of zeros using \(2*2\)
strides
<table class="image">
<caption align="bottom">Figure 1.5: Computing the output values of a discrete convolution for 
  \(N=2\)  \(i_1 = i_2 = 5\),  \(k_1 = k_2 = 3\) ,  \(s_1 = s_2 = 2\),  \(p_1 = p_2 = 1\)</caption>
<tr><td><img src="../assets/convolution-2.2.png" style="max-width: 100%; height: auto;" alt="Figure 1.5: Computing the output values of a discrete convolution for 
  \\(N=2\\)  \\(i\_1 = i\_2 = 5\\),  \\(k\_1 = k\_2 = 3\\) ,  \\(s\_1 = s\_2 = 2\\),  \\(p\_1 = p\_2 = 1\\)"/></td></tr>
</table></p></li>
<li><p>The strides constitute a form of <strong><em>subsampling</em></strong>. Strides can be
viewed as much of the output is retained.  For instance, moving the
kernel by hops of two is equivalent to moving the kernel by hops
of one, but retain only odd output elements (Figure 1.6)
<table class="image">
<caption align="bottom">Figure 1.6: An alternative way of viewing strides. Instead of 
translating the \(3*3\) kernel by increments of \(s=2\) (left), the kernel is translated by increments of \(1\) and only odd numbered output elements are retained</caption>
<tr><td><img src="../assets/convolution-2.3.png" style="max-width: 100%; height: auto;" alt="Figure 1.6: An alternative way of viewing strides. Instead of 
translating the \\(3\*3\\) kernel by increments of \\(s=2\\) (left), the kernel is translated by increments of \\(1\\) and only odd numbered output elements are retained"/></td></tr>
</table></p></li>
</ul>

<p><a name="L4"></a></p>

<h2>Convolutional Arithmetic</h2>

<p>The analysis of relationship between convolutional layer properties is
eased by the fact that they don&#39;t interact across axes, i.e., the choice
of kernel size, stride and zero padding along the axis \(j\) only affects
the output size along the axis \(j\)</p>

<p>The following simplified settings are used to analyse the convolution
layer properties</p>

<ul>
<li>\(2\)-\(D\) discrete convolutions (\(N = 2\))</li>
<li>square inputs (\(i_1 = i_2 = i\))</li>
<li>square kernel size (\(k_1 = k_2 = k\))</li>
<li>same strides along both axes (\(s_1 = s_2 = s\))</li>
<li>same zero padding along both axes (\(p_1 = p_2 = p\))</li>
</ul>

<p><a name="L41"></a></p>

<h3>No Zero Padding, Unit Strides</h3>

<p>The simplest case to analyse is when the kernel just slides across every position of the input (i.e., \(s = 1\) and \(p = 0\))
<table class="image">
<caption align="bottom">Figure 1.7: No Padding, Unit Strides</caption>
<tr><td><img src="../assets/no_padding_no_strides.png" style="max-width: 100%; height: auto;" alt="Figure 1.7: No Padding, Unit Strides"/></td></tr>
</table></p>

<p>Lets define the output size resulting from this setting</p>

<ul>
<li>The output size is the number of possible placements of the
kernel on the input</li>
<li>Lets consider the width axis: the kernel starts on the
leftmost part of the input feature map and slides by steps
of one until it touches the right side of the input</li>
<li>The size of the output will be equal to number of steps made,
plus one</li>
</ul>

<p><strong>Relationship-1</strong>: <em>For any \(i\), \(k\), and for \(s=1\) and \(p=0\),</em></p>

<p>$$o = (i-k)+1$$</p>

<ul>
<li>Figure 1.7 provides an example for \(i=4\), \(k = 3\), \(s=1\), therefore output size \(o = (4-3) + 1 = 2\) along each axis</li>
</ul>

<p><a name="L42"></a></p>

<h3>Zero Padding, Unit Strides</h3>

<p>Lets consider zero padding only restricting stride \(s = 1\). The effect of
zero padding increases the size of the input from \(i\) to \(i+2p\)
<table class="image">
<caption align="bottom">Figure 1.8: Zero Padding, Unit Strides</caption>
<tr><td><img src="../assets/arbitrary_padding_no_strides.PNG" style="max-width: 100%; height: auto;" alt="Figure 1.8: Zero Padding, Unit Strides"/></td></tr>
</table></p>

<p><strong>Relationship-2</strong>: <em>For any \(i\), \(k\), \(p\) and for \(s=1\),</em>
$$o = (i-k)+ 2p + 1$$</p>

<ul>
<li>Figure 1.8  provides an example for \(i = 5\), \(k = 4\) and \(p = 2\),
therefore output size \(o = (5-4) + 2*2 + 1 = 6\).</li>
</ul>

<p><a name="L43"></a></p>

<h3>Half padding</h3>

<p>Some times we require the output size of convolution to be same as input
size (i.e., \(o=i\)). In order for \(o=i\), we use \(p = \lfloor \dfrac{k}{2} \rfloor\)
<table class="image">
<caption align="bottom">Figure 1.9: Half Padding, Unit Strides</caption>
<tr><td><img src="../assets/same_padding_no_strides.PNG" style="max-width: 100%; height: auto;" alt="Figure 1.9: Half Padding, Unit Strides"/></td></tr>
</table></p>

<p><strong>Relationship-3</strong>: For any \(i\), and for odd \(k\) (\(k = 2n + 1\), \(n \in N\)), \(s=1\), and \(p = \lfloor \dfrac{k}{2} \rfloor = n\) 
$$o = (i+2\lfloor \dfrac{k}{2} \rfloor)-(k - 1)$$
$$o = (i + 2n - 2n)$$
$$o = i$$</p>

<ul>
<li>Figure 1.9  provides an example for \(i = 5\), \(k = 3\) hence \(p = 2\),
therefore output size \(o = (5+2*\lfloor \dfrac{3}{2} \rfloor) - (3 - 1) = 5 + 2 * 1 - 2 = 5\).</li>
</ul>

<p><a name="L44"></a></p>

<h3>Full padding</h3>

<p>Some times we require the output size of convolution to be of larger size than as input. But, convolution always decreases 
the size of the output if there is no extra padding to the input, so we can do some extra zero padding to the input.
<table class="image">
<caption align="bottom">Figure 1.10: Full Padding, Unit Strides</caption>
<tr><td><img src="../assets/full_padding_no_strides.PNG" style="max-width: 100%; height: auto;" alt="Figure 1.10: Full Padding, Unit Strides"/></td></tr>
</table></p>

<p><strong>Relationship-4</strong>: For any \(i\), and \(k\), \(s=1\), and \(p = k - 1\) 
$$o = (i + 2(k - 1) - (k - 1)$$
$$o = (i + (k - 1)$$</p>

<ul>
<li>Figure 1.10  provides an example for \(i = 5\), \(k = 3\) hence \(p = 2\),
therefore output size \(o = 5 + 3 - 1 = 7\).</li>
</ul>

<p><a name="L45"></a></p>

<h3>No zero padding, non-unit strides</h3>

<p>All relationships which we saw till now are unit-strided convolutions. In order to understand the effect of non-unit strides, 
lets ignore padding for now.</p>

<table class="image">
<caption align="bottom">Figure 1.11: No zero padding, non-unit Strides</caption>
<tr><td><img src="../assets/no_padding_strides.PNG" style="max-width: 100%; height: auto;" alt="Figure 1.11: No zero padding, non-unit Strides"/></td></tr>
</table>

<ul>
<li>As we discussed before, output size can be defined in terms of the number of possible placements of the kernel on the input.
If you consider only width axis, the size of the output is equal to the number of steps made, plus one, accounting for the initial position of the kernel.
The same logic applies for height axis.</li>
</ul>

<p><strong>Relationship-5</strong>: For any \(i\), \(k\), \(s\), and  for \(p = 0\) 
$$o = \lfloor \dfrac{i - k}{s} \rfloor + 1$$</p>

<ul>
<li><p>Figure 1.11  provides an example for \(i = 5\), \(k = 3\), \(p = 0\) and \(s = 2\),
therefore output size \(o = \lfloor \dfrac{5 - 3}{2} \rfloor + 1 = 2\).</p></li>
<li><p><strong><em>NOTE</em></strong> : The floor function in relationship-5 accounts for the fact that sometimes input size is such that kernel 
would not be able to reach all the input units.</p></li>
<li><p>Figure 1.12 illustrates this
<table class="image">
<caption align="bottom">Figure 1.12 Arbitrary padding and Strides</caption>
<tr><td><img src="../assets/arbitrary_padding_strides.PNG" style="max-width: 100%; height: auto;" alt="Figure 1.12 Arbitrary padding and Strides"/></td></tr>
</table></p></li>
</ul>

<p><a name="L46"></a></p>

<h3>Zero padding, non-unit strides</h3>

<p>This is the more general case, convolving over a zero padded input using a non-unit strides. 
We can derive by applying relationship-5 on effective input size of \(i + 2p\)</p>

<p><strong>Relationship-6</strong>: For any \(i\), \(k\), \(s\), and \(p\) 
$$o = \lfloor \dfrac{i + 2p - k}{s} \rfloor + 1$$
<table class="image">
<caption align="bottom">Figure 1.13 Arbitrary padding and Strides</caption>
<tr><td><img src="../assets/arbitrary_padding_strides-1.PNG" style="max-width: 100%; height: auto;" alt="Figure 1.13 Arbitrary padding and Strides"/></td></tr>
</table></p>

<ul>
<li>Figure 1.13  provides an example for \(i = 5\), \(k = 3\), \(s = 2\) and \(p = 1\),
therefore output size \(o = \lfloor \dfrac{5 + 2*1 - 3}{2} \rfloor + 1 = 5\).</li>
<li>Figure 1.12  provides an example for \(i = 6\), \(k = 3\), \(s = 2\) and \(p = 1\),
therefore output size \(o = \lfloor \dfrac{6 + 2*1 - 3}{2} \rfloor + 1 = 5\).</li>
</ul>

<p>Observe that even though both has different size inputs \(i = 5\) and \(i = 6\), output size after convolution is same \(o = 5\) for both.
As discussed before, this is due to the fact that kernel is not able to reach all the input units. </p>

<p><em>continued in <a href="http://localhost:4000/convolution-2/">part 2</a></em></p>

<p><a name="LR"></a></p>

<h2>References</h2>

<p>[1] <a href="https://arxiv.org/abs/1603.07285">A guide to convolution arithmetic for deep learning</a></p>

</div>

<div class="related">
  <h2>More Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/netsurgery/">
            Net Surgery in Caffe
            <small>01 Jun 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/fcn-segmentation/">
            Image Semantic Segmentation Using Fully Convolutional Neural Network
            <small>01 Jun 2016</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/convolution-2/">
            Convolution Arithmetic in  Deep Learning Part 2
            <small>14 May 2016</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      </div>
    </div>

  </body>
</html>
